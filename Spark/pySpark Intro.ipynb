{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pySpark Introduction # \n",
    "\n",
    "The steps that Spark follows are:\n",
    "\n",
    "* Dataset is loaded and partitioned across the cluster.\n",
    "* Mapping operation is performed on the distributed environment(`map()`).\n",
    "* All the partitions are collapsed together to generate the result(`reduce()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get spark configuration ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.rdd.compress', u'True'),\n",
       " (u'spark.master', u'yarn-client'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.executor.cores', u'2'),\n",
       " (u'spark.app.name', u'PySparkShell')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark config\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Resilient Distributed Dataset(RDD) \n",
    "numbers = range(10)\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "# We cannot simply pring the RDD content as it is split into multiple partitions(default number of partitions is twice\n",
    "# the number of cores or CPUs). In our case we have 2 cores and therefore, 4 partitions.\n",
    "numbers_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print all the values of the RDD object\n",
    "numbers_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a specific number of values from RDD\n",
    "numbers_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'For the latest information about Hadoop, please visit our website at:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in a text file from HDFS and print the first line(file is delimited using newline characters)\n",
    "sc.textFile(\"hdfs:///datasets/hadoop_git_readme.txt\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'For the latest information about Hadoop, please visit our website at:'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in a text file from the local filesystem\n",
    "sc.textFile(\"file:///home/vagrant/datasets/hadoop_git_readme.txt\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the content of the RDD in HDFS\n",
    "numbers_rdd.saveAsTextFile(\"hdfs:///tmp/numbers_1_10.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "-rw-r--r--   1 vagrant supergroup          0 2016-10-06 21:44 /tmp/numbers_1_10.txt/_SUCCESS\r\n",
      "-rw-r--r--   1 vagrant supergroup          4 2016-10-06 21:44 /tmp/numbers_1_10.txt/part-00000\r\n",
      "-rw-r--r--   1 vagrant supergroup          4 2016-10-06 21:44 /tmp/numbers_1_10.txt/part-00001\r\n",
      "-rw-r--r--   1 vagrant supergroup          4 2016-10-06 21:44 /tmp/numbers_1_10.txt/part-00002\r\n",
      "-rw-r--r--   1 vagrant supergroup          8 2016-10-06 21:44 /tmp/numbers_1_10.txt/part-00003\r\n"
     ]
    }
   ],
   "source": [
    "# List the content of the file that the content was saved to\n",
    "!hdfs dfs -ls /tmp/numbers_1_10.txt\n",
    "'''\n",
    "Spark writes one file for each partition exactly as MapReduce writing one file for each reducer. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "By using coalesce we can explicitly specify the number of partitions we want to split our data into. \n",
    "'''\n",
    "# Store our RDD in a standalone partition and when saved, should produce one output file.\n",
    "numbers_rdd.coalesce(1).saveAsTextFile(\"hdfs:///tmp/numbers_1_10_one_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 vagrant supergroup          0 2016-10-06 21:49 /tmp/numbers_1_10_one_file.txt/_SUCCESS\r\n",
      "-rw-r--r--   1 vagrant supergroup         20 2016-10-06 21:49 /tmp/numbers_1_10_one_file.txt/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "# List the content of the file that the content was saved to\n",
    "!hdfs dfs -ls /tmp/numbers_1_10_one_file.txt\n",
    "'''\n",
    "Data was stored in 1 partition\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of transformations and actions to show Lazy behavior ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Our objective in this example is to square the values contained in an RDD and then sum them up\n",
    "\n",
    "Mapping: This is done using the map() function is lazily evaluated.\n",
    "Reducing: This is done using reduce() and is an action and is therefore not lazy.\n",
    "'''\n",
    "# Step 1: Mapping\n",
    "# Define the funtion that returns the square of the input argument\n",
    "def sq(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the square of the elements in RDD using the 'map' function\n",
    "# We need to call collect as map is a lazy function and will not return an output unless used with collect()\n",
    "numbers_rdd.map(sq).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the same using the lambda function\n",
    "numbers_rdd.map(lambda x: x**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Reducing using reduce()\n",
    "numbers_rdd.map(lambda x: x**2).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example introducing key-value pairs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Our objective in this example is to find the sums of odd and even numbers in our RDD seperately.\n",
    "'''\n",
    "# Step 1: Define the function\n",
    "def tag(x):\n",
    "    return 'even' if x%2 == 0 else 'odd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('even', 0),\n",
       " ('odd', 1),\n",
       " ('even', 2),\n",
       " ('odd', 3),\n",
       " ('even', 4),\n",
       " ('odd', 5),\n",
       " ('even', 6),\n",
       " ('odd', 7),\n",
       " ('even', 8),\n",
       " ('odd', 9)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the reuslts\n",
    "numbers_rdd.map(lambda x: (tag(x), x) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('even', 20), ('odd', 25)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "To get the squares of odds and evens together, we use reduceByKey(fxn). \n",
    "\n",
    "It has 2 steps:\n",
    "- Aggregates the input RDD by key.\n",
    "- Applies the reduce fxn to values of each group.\n",
    "'''\n",
    "numbers_rdd.map(lambda x: (tag(x), x) ) \\\n",
    "           .reduceByKey(lambda a,b: a + b) \\\n",
    "           .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix ##\n",
    "\n",
    "Some useful transformations in spark are:\n",
    "\n",
    "* map(fxn)\n",
    "* flatmap(fxn)\n",
    "* filter(fxn)\n",
    "* sample(withReplacement, fraction, seed)\n",
    "* distinct()\n",
    "* coalesce(numPartitions)\n",
    "* repartition(numPartitions)\n",
    "* groupsByKey()\n",
    "* reduceByKey()\n",
    "* sortByKey(ascending)\n",
    "* union(otherRDD)\n",
    "* intersection(otherRDD)\n",
    "* join(otherRDD) [leftOuterJoin, rightOuterJoin, fullOuterJoin]\n",
    "* cartesian()\n",
    "\n",
    "Some useful actions in spark are:\n",
    "\n",
    "* reduce(fxn)\n",
    "* count()\n",
    "* countByKey()\n",
    "* collect()\n",
    "* first()\n",
    "* take(n)\n",
    "* takeSample(withReplacement, n, seed)\n",
    "* takeOrdered(n, ordering)\n",
    "* saveAsTextFile(path)\n",
    "\n",
    "Useful methods that are neither transformations nor actions:\n",
    "\n",
    "* cache()\n",
    "* persist(storage) [starage can be memory, disk or both]\n",
    "* unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
