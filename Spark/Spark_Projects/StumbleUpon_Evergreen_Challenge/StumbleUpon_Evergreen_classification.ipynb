{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Scala tutorials: \n",
    "\n",
    "* http://blog.codacy.com/2015/07/03/how-to-learn-scala/\n",
    "* http://www.newthinktank.com/2015/08/learn-scala-one-video/\n",
    "* https://learnxinyminutes.com/docs/scala/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification problem statement # \n",
    "\n",
    "Here we will use the https://www.kaggle.com/c/stumbleupon problem to classify web pages as:\n",
    "\n",
    "* **Evergreen**: Pages that are persistently popular labeled as 1.\n",
    "* **Ephemeral**: Pages that are popular for a short amount of time labeled as 0.\n",
    "\n",
    "We will download train.tsv which is the training set and contains 7,395 urls. \n",
    "\n",
    "The feature set is as follows:\n",
    "\n",
    "<img src = \"evergreen1.png\">\n",
    "<img src = \"evergreen2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data preprocessing #\n",
    "\n",
    "Load the data into `rawData`\n",
    "\n",
    "`scala> val rawData = sc.textFile(\"/Users/adarshnair/spark-2.0.1-bin-hadoop2.7/spark_projects/Classification/train_noheader.tsv\")`\n",
    "\n",
    "** Preview the data **\n",
    "\n",
    "`scala> rawData.first\n",
    "res4: String = \"url\"\t\"urlid\"\t\"boilerplate\"\t\"alchemy_category\"\t\"alchemy_category_score\"\t\"avglinksize\"\t\"commonlinkratio_1\"\t\"commonlinkratio_2\"\t\"commonlinkratio_3\"\t\"commonlinkratio_4\"\t\"compression_ratio\"\t\"embed_ratio\"\t\"framebased\"\t\"frameTagRatio\"\t\"hasDomainLink\"\t\"html_ratio\"\t\"image_ratio\"\t\"is_news\"\t\"lengthyLinkDomain\"\t\"linkwordscore\"\t\"news_front_page\"\t\"non_markup_alphanum_characters\"\t\"numberOfLinks\"\t\"numwords_in_url\"\t\"parametrizedLinkRatio\"\t\"spelling_errors_ratio\"\t\"label\"`\n",
    "\n",
    "The first line of the data are the column headers.\n",
    "\n",
    "** Remove the column headers **\n",
    "\n",
    "We can do this using the `sed` command.\n",
    "\n",
    "`Adarshs-MacBook-Pro:Classification adarshnair$ sed 1d train.tsv > train_noheader.tsv`\n",
    "\n",
    "Now preview the data after updating the rawData file to point to train_noheader.tsv\n",
    "\n",
    "`scala> val rawData = sc.textFile(\"/Users/adarshnair/spark-2.0.1-bin-hadoop2.7/spark_projects/Classification/train_noheader.tsv\")\n",
    "rawData: org.apache.spark.rdd.RDD[String] = /Users/adarshnair/spark-2.0.1-bin-hadoop2.7/spark_projects/Classification/train_noheader.tsv MapPartitionsRDD[5] at textFile at <console>:24`\n",
    "\n",
    "`scala> rawData.first\n",
    "res5: String = \"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\"\t\"4042\"\t\"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees in its crystal ...`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split the data on the \\t **\n",
    "\n",
    "`scala> val records = rawData.map(line => line.split(\"\\t\"))`\n",
    "\n",
    "Preview data:\n",
    "\n",
    "`scala> records.first\n",
    "res6: Array[String] = Array(\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\", \"4042\", \"{\"\"title\"\":\"\"IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries\"\",\"\"body\"\":\"\"A sign stands outside the International Business Machines Corp IBM Almaden Research Center campus in San Jose California Photographer Tony Avelar Bloomberg Buildings stand at the International Business Machines Corp IBM Almaden Research Center campus in the Santa Teresa Hills of San Jose California Photographer Tony Avelar Bloomberg By 2015 your mobile phone will project a 3 D image of anyone who calls and your laptop will be powered by kinetic energy At least that s what International Business Machines Corp sees ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Data cleaning **\n",
    "\n",
    "* Trim extra quoatation marks\n",
    "* Replace missing values which have '?' with a '0'\n",
    "* Set the label to `label` which is the last column\n",
    "* Set the features to `features` for columns 5 to 25\n",
    "* Wrap the `label` and `features` using `LabeledPoint` which converts the features into an MLlib vector. (http://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point)\n",
    "\n",
    "`import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors`\n",
    "\n",
    "\n",
    "`scala> val data = records.map { r =>\n",
    "     | val trimmed  = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "     | val label = trimmed(r.size - 1).toInt\n",
    "     | val features = trimmed.slice(4, r.size - 1).map(x => if (x == \"?\") 0.0 else x.toDouble )\n",
    "     | LabeledPoint(label, Vectors.dense(features))\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cache the data **\n",
    "\n",
    "`scala> data.cache`\n",
    "\n",
    "** Count the number of rows or webapages **\n",
    "\n",
    "`scala> val numData = data.count\n",
    "numData: Long = 7395  `\n",
    "\n",
    "** Data cleaning specific to when we use data for Naive Bayes classifier **\n",
    "\n",
    "The NB classifier cannot use features which have negative values. We will create a version of the dataset with the negative values removed by adding `.map(d => if (d < 0) 0.0 else d)`\n",
    "\n",
    "`scala> val nbData = records.map { r =>\n",
    "     | val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "     | val label = trimmed(r.size - 1).toInt\n",
    "     | val features = trimmed.slice(4, r.size -1).map(x => if (x == \"?\") 0.0 else x.toDouble).map(x => if (x < 0) 0.0 else x)\n",
    "     | LabeledPoint(label, Vectors.dense(features))\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Training classification models #\n",
    "\n",
    "We will train the data using 4 classification models.\n",
    "\n",
    "`scala> val numIterations = 10\n",
    "numIterations: Int = 10`\n",
    "\n",
    "`scala> val maxTreeDepth = 5\n",
    "maxTreeDepth: Int = 5`\n",
    "\n",
    "## Step 2.1: Logistic Regression ##\n",
    "\n",
    "`scala> import org.apache.spark.mllib.classification.LogisticRegressionWithSGD`\n",
    "\n",
    "We set the number of iterations to 10.\n",
    "\n",
    "`scala> val lrModel = LogisticRegressionWithSGD.train(data, numIterations)`\n",
    "\n",
    "## Step 2.2: SVM ##\n",
    "\n",
    "`scala> import org.apache.spark.mllib.classification.SVMWithSGD`\n",
    "\n",
    "`scala> val svmModel = SVMWithSGD.train(data, numIterations)`\n",
    "\n",
    "\n",
    "## Step 2.3: Naive Bayes ##\n",
    "\n",
    "`scala> import org.apache.spark.mllib.classification.NaiveBayes`\n",
    "\n",
    "We will use the nbData which has the negative values for features replaced with 0.\n",
    "\n",
    "`scala> val nbModel = NaiveBayes.train(nbData) `\n",
    "\n",
    "## Step 2.4: Decision Trees ##\n",
    "\n",
    "`scala> import org.apache.spark.mllib.tree.DecisionTree`\n",
    "\n",
    "`scala> import org.apache.spark.mllib.tree.configuration.Algo`\n",
    "\n",
    "`scala> import org.apache.spark.mllib.tree.impurity.Entropy`\n",
    "\n",
    "We will set the maxTreeDepth to 5, the mode to Algo.Classification and the impurity measure to Entropy.\n",
    "\n",
    "`scala> val dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.a: Make predictions and check performance using our trained classification models #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1.a: Logistic regression predictions ##\n",
    "\n",
    "Prediction on a single datapoint.\n",
    "\n",
    "`scala> val dataPoint = data.first`\n",
    "\n",
    "`scala> val prediction = lrModel.predict(dataPoint.features)\n",
    "prediction: Double = 1.0`\n",
    "\n",
    "Compare this prediction with the actual label of the first datapoint.\n",
    "\n",
    "`scala> val trueLabel = dataPoint.label\n",
    "trueLabel: Double = 0.0`\n",
    "\n",
    "Thus the model did not get it right for the first datapoint.\n",
    "\n",
    "Make predictions on the entire training set.\n",
    "\n",
    "`scala> val predictions = lrModel.predict(data.map(x => x.features))`\n",
    "\n",
    "Check the values of the first five predictions.\n",
    "\n",
    "`scala> predictions.take(5)\n",
    "16/10/19 14:38:23 WARN Executor: 1 block locks were not released by TID = 87:\n",
    "[rdd_7_0]\n",
    "res8: Array[Double] = Array(1.0, 1.0, 1.0, 1.0, 1.0)`\n",
    "\n",
    "## Step 3.1.b: Logistic regression performance ##\n",
    "\n",
    "\n",
    "** Accuracy **\n",
    "\n",
    "This is the ratio of the number of correctly clasified instances divided by the total.\n",
    "\n",
    "`scala> val lrTotalCorrect = data.map { point =>\n",
    "     | if (lrModel.predict(point.features) == point.label) 1 else 0\n",
    "     | }.sum\n",
    "lrTotalCorrect: Double = 3806.0`\n",
    "\n",
    "`scala> val lrAccuracy = lrTotalCorrect / numData\n",
    "lrAccuracy: Double = 0.5146720757268425`\n",
    "\n",
    "The accuracy os 51.46%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: SVM predictions and performance ##\n",
    "\n",
    "** Predictions **\n",
    "\n",
    "`scala> val svmTotalCorrect = data.map { x =>\n",
    "     | if (svmModel.predict(x.features) == x.label) 1 else 0\n",
    "     | }.sum\n",
    "svmTotalCorrect: Double = 3806.0 `\n",
    "\n",
    "** Accuracy ** \n",
    "\n",
    "`scala> val svmAccuracy = svmTotalCorrect / numData\n",
    "svmAccuracy: Double = 0.5146720757268425`\n",
    "\n",
    "The accuracy is 51.46%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Naive Bayes predictions  and performance ##\n",
    "\n",
    "** Predictions **\n",
    "\n",
    "`scala> val nbTotalCorrect = nbData.map { point =>\n",
    "     | if (nbModel.predict(point.features) == point.label) 1 else 0\n",
    "     | }.sum\n",
    "nbTotalCorrect: Double = 4292.0 `\n",
    "\n",
    "** Accuracy **\n",
    "\n",
    "`scala> val nbAccuracy = nbTotalCorrect / numData\n",
    "nbAccuracy: Double = 0.5803921568627451`\n",
    "\n",
    "The accuracy is 58.09%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Decision tree predictions and performance ##\n",
    "\n",
    "** Predictions **\n",
    "\n",
    "`scala> val dtTotalCorrect = data.map { point =>\n",
    "     | val score = dtModel.predict(point.features)\n",
    "     | val predicted = if (score > 0.5) 1 else 0 \n",
    "     | if (predicted == point.label) 1 else 0\n",
    "     | }.sum\n",
    "dtTotalCorrect: Double = 4794.0`\n",
    "\n",
    "** Accuracy **\n",
    "\n",
    "`scala> val dtAccuracy = dtTotalCorrect / numData\n",
    "dtAccuracy: Double = 0.6482758620689655`\n",
    "\n",
    "The accuracy is 64.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.b: Additional performance metrics ##\n",
    "\n",
    "** Precision **\n",
    "\n",
    "`TP / (TP + FP)`\n",
    "\n",
    "** Recall/Sensitivity/True Positive Rate**\n",
    "\n",
    "`TP / (TP + FN)`\n",
    "\n",
    "** Area under Precision-Recall curve **\n",
    "\n",
    "A value of 1 denotes a perfect classifier.\n",
    "\n",
    "** False Positive Rate **\n",
    "\n",
    "`FP / (FP + TN)`\n",
    "\n",
    "** ROC curve/Area under ROC curve(AUC)**\n",
    "\n",
    "The graph that plots the classifiers performance tradeoff of True positive rate against False positive rate. An AUC value of 1 denotes a perfect classifier.\n",
    "\n",
    "** Computing Area under PR curve and AUC curve for Logistic Regression and SVM models **\n",
    "\n",
    "`scala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics`\n",
    "\n",
    "`scala> val metrics = Seq(lrModel, svmModel).map { model => \n",
    "     | val scoreAndLabels = data.map { point =>\n",
    "     | (model.predict(point.features), point.label)\n",
    "     | }\n",
    "     | val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "     | (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "     | }\n",
    "metrics: Seq[(String, Double, Double)] = List((LogisticRegressionModel,0.7567586293858841,0.5014181143280931), (SVMModel,0.7567586293858841,0.5014181143280931))`\n",
    "\n",
    "We get the following values:\n",
    "* LogisticRegressionModel - PR Curve: 0.75, AUC: 0.501\n",
    "* SVMModel - PR Curve: 0.756, AUC: 0.501\n",
    "\n",
    "** Computing Area under PR curve and AUC curve for Naive Bayes model **\n",
    "\n",
    "`scala> val nbMetrics = Seq(nbModel).map{ model =>\n",
    "     | val scoreAndLabels = nbData.map { point =>\n",
    "     | val score = model.predict(point.features)\n",
    "     | (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "     | }\n",
    "     | val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "     | (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "     | }\n",
    "nbMetrics: Seq[(String, Double, Double)] = List((NaiveBayesModel,0.6808510815151734,0.5835585110136261))`\n",
    "\n",
    "We get the following values:\n",
    "* NaiveBayesModel - PR Curve: 0.68, AUC: 0.58\n",
    "\n",
    "** Computing Area under PR curve and AUC curve for Decision Tree model **\n",
    "\n",
    "`scala> val dtMetrics = Seq(dtModel).map{ model =>\n",
    "     | val scoreAndLabels = data.map { point =>\n",
    "     | val score = model.predict(point.features)\n",
    "     | (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "     | }\n",
    "     | val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "     | (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "     | }\n",
    "dtMetrics: Seq[(String, Double, Double)] = List((DecisionTreeModel,0.7430805993331199,0.6488371887050935))`\n",
    "\n",
    "We get the following values:\n",
    "* DecisionTreeModel - PR Curve: 0.74, AUC: 0.648\n",
    "\n",
    "** Final output with all values: **\n",
    "\n",
    "`scala> val allMetrics = metrics ++ nbMetrics ++ dtMetrics\n",
    "allMetrics: Seq[(String, Double, Double)] = List((LogisticRegressionModel,0.7567586293858841,0.5014181143280931), (SVMModel,0.7567586293858841,0.5014181143280931), (NaiveBayesModel,0.6808510815151734,0.5835585110136261), (DecisionTreeModel,0.7430805993331199,0.6488371887050935))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> allMetrics.foreach{ case (m, pr, roc) => \n",
    "     | println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\") \n",
    "     | }`\n",
    "     \n",
    "`LogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
    "SVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
    "NaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%\n",
    "DecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature Standardization # \n",
    "\n",
    "## Step 4.1: Analyse features ##\n",
    "\n",
    "Most models make the assumption that the features are normally distributed. To investigate this, we represent the feature vectors as a distributed matrix in MLlib using the `RowMatrix` class. `RowMatrix` is an RDD made up of vector where each vector is row of our matrix.\n",
    "\n",
    "`scala> import org.apache.spark.mllib.linalg.distributed.RowMatrix`\n",
    "\n",
    "Create `vectors` variable which has our features.\n",
    "\n",
    "`scala> val vectors = data.map(lp => lp.features)`\n",
    "\n",
    "Create `matrix` variable which is the matrix of our features.\n",
    "\n",
    "`scala> val matrix = new RowMatrix(vectors)`\n",
    "\n",
    "** Compute summary stats of our features using `computeColumnSummaryStatistics()` ** \n",
    "\n",
    "`scala> val matrixSummary = matrix.computeColumnSummaryStatistics()`\n",
    "\n",
    "Using the matrixSummary object we can find the mean, min, max, variance and numNonzeros.\n",
    "\n",
    "`scala> println(matrixSummary.mean)`\n",
    "\n",
    "`scala> println(matrixSummary.max)`\n",
    "\n",
    "`scala> println(matrixSummary.variance)`\n",
    "\n",
    "`scala> println(matrixSummary.numNonzeros)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2: Standardize and scale features ##\n",
    "\n",
    "** Import the StandardScaler:**\n",
    "\n",
    "`scala> import org.apache.spark.mllib.feature.StandardScaler`\n",
    "\n",
    "It takes 2 arguments - withMean which when set to True will subtract the mean from the data and withStd which applies the standard deviation scaling.\n",
    "\n",
    "`scala> val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)`\n",
    "\n",
    "** Get the scaled data:**\n",
    "\n",
    "`scala> val scaledData = data.map(x => LabeledPoint(x.label, scaler.transform(x.features)))`\n",
    "\n",
    "** Unscaled features:** \n",
    "\n",
    "`scala> println(data.first.features)\n",
    "16/10/19 16:06:00 WARN Executor: 1 block locks were not released by TID = 180:\n",
    "[rdd_7_0]\n",
    "[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,\n",
    "0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,\n",
    "5424.0,170.0,8.0,0.152941176,0.079129575]`\n",
    "\n",
    "** Scaled features:**\n",
    "\n",
    "`scala> println(scaledData.first.features)\n",
    "16/10/19 16:06:10 WARN Executor: 1 block locks were not released by TID = 181:\n",
    "[rdd_7_0]\n",
    "[1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,\n",
    "-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,\n",
    "0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,\n",
    "0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,\n",
    "-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,\n",
    "-0.2788207823137022]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3: Retrain Logistic Regression model using scaled features ##\n",
    "\n",
    "*Naive Bayes and Decision Trees are unaffected by scaling feautes.*\n",
    "\n",
    "** Train model using scaled features ** \n",
    "\n",
    "`scala> val lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)`\n",
    "\n",
    "** Get total correct predictions using scaled features **\n",
    "\n",
    "`scala> val lrTotalCorrectScaled = scaledData.map { point =>\n",
    "     | if (lrModelScaled.predict(point.features) == point.label) 1 else 0\n",
    "     | }.sum\n",
    "lrTotalCorrectScaled: Double = 4588.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Get accuracy **\n",
    "\n",
    "`scala> val lrAccuracyScaled = lrTotalCorrectScaled / numData\n",
    "lrAccuracyScaled: Double = 0.6204192021636241`\n",
    "\n",
    "\n",
    "** Make compare predictions with actual labels **\n",
    "\n",
    "`scala> val lrPredictionsVsTrue = scaledData.map { point => \n",
    "     | (lrModelScaled.predict(point.features), point.label) \n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Get Metrics object **\n",
    "\n",
    "`scala> val lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\n",
    "lrMetricsScaled: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@49a9766b`\n",
    "\n",
    "** PR Curve Score **\n",
    "\n",
    "`scala> val lrPr = lrMetricsScaled.areaUnderPR\n",
    "lrPr: Double = 0.7272540762713375`\n",
    "\n",
    "** AUC Curve Score **\n",
    "\n",
    "`scala> val lrRoc = lrMetricsScaled.areaUnderROC\n",
    "lrRoc: Double = 0.6196629669112512v`\n",
    "\n",
    "Using scaled features we improved our AUC score from ~50% to 62%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Considering more features #\n",
    "\n",
    "In step 1 we considered only the numeric features to be a part of our features set(columns 5-25) by doing \n",
    "\n",
    "` val features = trimmed.slice(4, r.size - 1).map(x => if (x == \"?\") 0.0 else x.toDouble )`\n",
    "\n",
    "Now we shall consider the **alchemy_category** feature as well, which is the 4th feature.\n",
    "\n",
    "As it is a string feature we will have to encode it, using the ** 1-of-k** encoding.\n",
    "\n",
    "## Step 5.1: 1-of-k encoding ##\n",
    "\n",
    "**1-of-k encode the categories using `zipWithIndex.toMap`**\n",
    "\n",
    "`scala> val categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\n",
    "categories: scala.collection.immutable.Map[String,Int] = Map(\"weather\" -> 0, \"sports\" -> 6, \"unknown\" -> 4, \"computer_internet\" -> 12, \"?\" -> 11, \"culture_politics\" -> 3, \"religion\" -> 8, \"recreation\" -> 2, \"arts_entertainment\" -> 9, \"health\" -> 5, \"law_crime\" -> 10, \"gaming\" -> 13, \"business\" -> 1, \"science_technology\" -> 7)`\n",
    "\n",
    "**Get the number of categories**\n",
    "\n",
    "`scala> val numCategories = categories.size\n",
    "numCategories: Int = 14`\n",
    "\n",
    "Create a vector of length 14 to represent this feature and assign a value of 1 for the index of the relevant relevant category for each data point.\n",
    "\n",
    "`scala> val dataCategories = records.map { r =>\n",
    "     | val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "     | val label = trimmed(r.size - 1).toInt\n",
    "     | val categoryIdx = categories(r(3))\n",
    "     | val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "     | categoryFeatures(categoryIdx) = 1.0\n",
    "     | val otherFeatures = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
    "     | val features = categoryFeatures ++ otherFeatures\n",
    "     | LabeledPoint(label, Vectors.dense(features))\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the new feature set for the first webpage:\n",
    "\n",
    "`scala> println(dataCategories.first.features)\n",
    "[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,\n",
    "0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,\n",
    "8.0,0.152941176,0.079129575]`\n",
    "\n",
    "The first feature is the category feature with a value 1 for the category that webpage belongs to and 0 for every other category.\n",
    "\n",
    "## Step 5.2: Standardize the features ##\n",
    "\n",
    "** Standardize the features **\n",
    "\n",
    "`scala> val scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp => lp.features))`\n",
    "\n",
    "`scala> val scaledDataCats = dataCategories.map(lp => LabeledPoint(lp.label, scalerCats.transform(lp.features)))`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.3: Train model on scaled data and expanded feature set##\n",
    "\n",
    "** Train model **\n",
    "\n",
    "`scala> val lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations)`\n",
    "\n",
    "## Step 5.4: Performance metrics ##\n",
    "\n",
    "** Make predictions **\n",
    "\n",
    "`scala> val lrTotalCorrectScaledCats = scaledDataCats.map { point =>\n",
    "     | if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0\n",
    "     | }.sum\n",
    "lrTotalCorrectScaledCats: Double = 4923.0`\n",
    "\n",
    "** Get accuracy of predictions **\n",
    "\n",
    "`scala> val lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\n",
    "lrAccuracyScaledCats: Double = 0.6657200811359026`\n",
    "\n",
    "** Get PR Curve value **\n",
    "\n",
    "`scala> val lrPredictionsVsTrueCats = scaledDataCats.map { point => \n",
    "     | (lrModelScaledCats.predict(point.features), point.label) \n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> val lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)`\n",
    "\n",
    "`scala> val lrPrCats = lrMetricsScaledCats.areaUnderPR\n",
    "lrPrCats: Double = 0.7579640787676577`\n",
    "\n",
    "** Get AUC value **\n",
    "\n",
    "`scala> val lrRocCats = lrMetricsScaledCats.areaUnderROC\n",
    "lrRocCats: Double = 0.6654826844243996`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Collate performance metric values **\n",
    "\n",
    "`scala> println(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\") \n",
    "LogisticRegressionModel\n",
    "Accuracy: 66.5720%\n",
    "Area under PR: 75.7964%\n",
    "Area under ROC: 66.5483%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Using the correct form of data #\n",
    "\n",
    "When the data has features that are both categorical(0,1 values) and frequency data(count values which can range from 0 to n) it can affect the performance of our model. To illustrate this we will train the Naive Bayes model using just the categorical data(column 4).\n",
    "\n",
    "`scala> val dataNB = records.map { r =>\n",
    "     | val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "     | val label = trimmed(r.size - 1).toInt\n",
    "     | val categoryIdx = categories(r(3))\n",
    "     | val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "     | categoryFeatures(categoryIdx) = 1.0\n",
    "     | LabeledPoint(label, Vectors.dense(categoryFeatures))\n",
    "     | }`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train model **\n",
    "\n",
    "`scala> val nbModelCats = NaiveBayes.train(dataNB)`\n",
    "\n",
    "** Get performance metrics(accuracy) for newly trained model **\n",
    "\n",
    "`scala> val nbTotalCorrectCats = dataNB.map { point =>\n",
    "     | if (nbModelCats.predict(point.features) == point.label) 1 else 0\n",
    "     | }.sum\n",
    "nbTotalCorrectCats: Double = 4508.0`\n",
    "\n",
    "`scala> val nbAccuracyCats = nbTotalCorrectCats / numData\n",
    "nbAccuracyCats: Double = 0.6096010818120352`\n",
    "\n",
    "`scala> val nbPredictionsVsTrueCats = dataNB.map { point => \n",
    "     | (nbModelCats.predict(point.features), point.label) \n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Get PR Curve, AUC curve values **\n",
    "\n",
    "`scala> val nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)`\n",
    "\n",
    "`scala> val nbPrCats = nbMetricsCats.areaUnderPR\n",
    "nbPrCats: Double = 0.7405222106704076                                           \n",
    "\n",
    "scala> val nbRocCats = nbMetricsCats.areaUnderROC\n",
    "nbRocCats: Double = 0.6051384941549446`\n",
    "\n",
    "`scala> println(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\") \n",
    "NaiveBayesModel\n",
    "Accuracy: 60.9601%\n",
    "Area under PR: 74.0522%\n",
    "Area under ROC: 60.5138%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metric of the Naive Model rose by 2% points by using just the categorical feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Tuning parameters #\n",
    "\n",
    "## Step 7.1.a: Logistic Regression(SGD) parameters ##\n",
    "\n",
    "The arguments for logistic regression are as follows:\n",
    "\n",
    "* stepSize\n",
    "* numIterations\n",
    "* regParam\n",
    "* miniBatchFraction\n",
    "\n",
    "** Necessary imports: **\n",
    "\n",
    "`scala> import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.rdd.RDD`\n",
    "\n",
    "`scala> import org.apache.spark.mllib.optimization.Updater\n",
    "import org.apache.spark.mllib.optimization.Updater`\n",
    "\n",
    "`scala> import org.apache.spark.mllib.optimization.SimpleUpdater\n",
    "import org.apache.spark.mllib.optimization.SimpleUpdater`\n",
    "\n",
    "`scala> import org.apache.spark.mllib.optimization.L1Updater\n",
    "import org.apache.spark.mllib.optimization.L1Updater`\n",
    "\n",
    "`scala> import org.apache.spark.mllib.optimization.SquaredL2Updater\n",
    "import org.apache.spark.mllib.optimization.SquaredL2Updater`\n",
    "\n",
    "`scala> import org.apache.spark.mllib.classification.ClassificationModel\n",
    "import org.apache.spark.mllib.classification.ClassificationModel`\n",
    "\n",
    "** Helper function to train logistic regression model **\n",
    "\n",
    "We feed the function the input, along with arguments for stepSize, numIterations, regParam.\n",
    "\n",
    "`scala> def trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {\n",
    "     | val lr = new LogisticRegressionWithSGD\n",
    "     | lr.optimizer.setNumIterations(numIterations).setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n",
    "     | lr.run(input)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Helper function to calculate AUC metric **\n",
    "\n",
    "`scala> def createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {\n",
    "     | val scoreAndLabels = data.map { point =>\n",
    "     | (model.predict(point.features), point.label)\n",
    "     | }\n",
    "     | val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "     | (label, metrics.areaUnderROC)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cache the data to increase speed against multiple runs against the dataset ** \n",
    "\n",
    "`scala> scaledDataCats.cache`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.1.b: Tuning Logistic regression parameters ##\n",
    "\n",
    "We will be tuning 3 parameters:\n",
    "\n",
    "** Number of iterations **\n",
    "\n",
    "We will run the model on 1,5,10 and 50 iteratations and use the helper functions to train and find AUC scores for each.\n",
    "\n",
    "`scala> val iterResults = Seq(1, 5, 10, 50).map { param =>\n",
    "     | val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\n",
    "     | createMetrics(s\"$param iterations\", scaledDataCats, model)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> iterResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
    "1 iterations, AUC = 64.95%\n",
    "5 iterations, AUC = 66.62%\n",
    "10 iterations, AUC = 66.55%\n",
    "50 iterations, AUC = 66.81%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step size ** \n",
    "\n",
    "Step size decides how far in the direction of the steepest gradient the algorithm takes a step when updating the model weight vector after each training sample. We will try step sizes - 0.001, 0.01, 0.1, 1.0, 10.0.\n",
    "\n",
    "`scala> val stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "     | val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\n",
    "     | createMetrics(s\"$param step size\", scaledDataCats, model)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> stepResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
    "0.001 step size, AUC = 64.97%\n",
    "0.01 step size, AUC = 64.96%\n",
    "0.1 step size, AUC = 65.52%\n",
    "1.0 step size, AUC = 66.55%\n",
    "10.0 step size, AUC = 61.92%`\n",
    "\n",
    "The AUC improves as we go from a step size of 0.001 to 1.0 but falls at 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Updater **\n",
    "\n",
    "This argument controls the regularization. Regularization can help prevent over fitting by penalizing model complexity. \n",
    "\n",
    "When we have **low regularization, models tend to overfit**, ** when it is too high, models tend to underfit.**\n",
    "\n",
    "The forms of regularization available are:\n",
    "\n",
    "* SimpleUpdater - default for **Logistic Regression**, no regularization\n",
    "* SquaredL2Updater - default for ** SVMs **, squared L2 norm of the weight vector\n",
    "* L1Updater - L1 norm of the weight vector\n",
    "\n",
    "** We will use the SquaredL2Updater regularizer with values - 0.001, 0.01, 0.1, 1.0, 10.0 **\n",
    "\n",
    "`scala> val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "     | val model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "     | createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> regResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
    "0.001 L2 regularization parameter, AUC = 66.55%\n",
    "0.01 L2 regularization parameter, AUC = 66.55%\n",
    "0.1 L2 regularization parameter, AUC = 66.63%\n",
    "1.0 L2 regularization parameter, AUC = 66.04%\n",
    "10.0 L2 regularization parameter, AUC = 35.33%`\n",
    "\n",
    "With a high value for the Updater, we can see that the AUC value goes down drastically due to underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.2: Tuning Decision Tree parameters ##\n",
    "\n",
    "We will be tuning:\n",
    "\n",
    "* **maxDepth**: controls the complexity of the model\n",
    "* **impurity**: choose between `Gini` and `Entropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load libraries **\n",
    "\n",
    "`scala> import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "\n",
    "scala> import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "\n",
    "scala> import org.apache.spark.mllib.tree.impurity.Gini\n",
    "import org.apache.spark.mllib.tree.impurity.Gini`\n",
    "\n",
    "** Helper function to iterate through tuned parameters **\n",
    "\n",
    "`scala> def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {\n",
    "     | DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Iterating through different values for maxDepth with the Entropy impurity parameter**\n",
    "\n",
    "We will use 1, 2, 3, 4, 5, 10, 20 values for the maxDepth parameter.\n",
    "\n",
    "`scala> val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n",
    "     | val model = trainDTWithParams(data, param, Entropy)\n",
    "     | val scoreAndLabels = data.map { point =>\n",
    "     | val score = model.predict(point.features)\n",
    "     | (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "     | }\n",
    "     | val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "     | (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> dtResultsEntropy.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
    "1 tree depth, AUC = 59.33%\n",
    "2 tree depth, AUC = 61.68%\n",
    "3 tree depth, AUC = 62.61%\n",
    "4 tree depth, AUC = 63.63%\n",
    "5 tree depth, AUC = 64.88%\n",
    "10 tree depth, AUC = 76.26%\n",
    "20 tree depth, AUC = 98.45%`\n",
    "\n",
    "We get our best result at higher model complexity with 20 levels, but this is likely because we are over fitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Iterating through different values for maxDepth with the Gini impurity parameter**\n",
    "\n",
    "`scala> val dtResultsGini = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n",
    "     | val model = trainDTWithParams(data, param, Gini)\n",
    "     | val scoreAndLabels = data.map { point =>\n",
    "     | val score = model.predict(point.features)\n",
    "     | (if (score > 0.5) 1.0 else 0.0, point.label)\n",
    "     | }\n",
    "     | val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "     | (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> dtResultsGini.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
    "1 tree depth, AUC = 59.33%\n",
    "2 tree depth, AUC = 61.68%\n",
    "3 tree depth, AUC = 62.61%\n",
    "4 tree depth, AUC = 63.63%\n",
    "5 tree depth, AUC = 64.89%\n",
    "10 tree depth, AUC = 78.37%\n",
    "20 tree depth, AUC = 98.87%`\n",
    "\n",
    "We get very similar results with the Gini parameter as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7.3: Tuning Naive Bayes model ##\n",
    "\n",
    "Here we will tune the **`lambda`** parameter which controls the additive smoothing, which handles the case when a class and feature value do not occur together in the dataset.\n",
    "\n",
    "** Helper function to iterate through lambda values **\n",
    "\n",
    "`scala> def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\n",
    "     | val nb = new NaiveBayes\n",
    "     | nb.setLambda(lambda)\n",
    "     | nb.run(input)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Iterate through values for lambda **\n",
    "\n",
    "`scala> val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "     | val model = trainNBWithParams(dataNB, param)\n",
    "     | val scoreAndLabels = dataNB.map { point =>\n",
    "     | (model.predict(point.features), point.label)\n",
    "     | }\n",
    "     | val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "     | (s\"$param lambda\", metrics.areaUnderROC)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> nbResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
    "0.001 lambda, AUC = 60.51%\n",
    "0.01 lambda, AUC = 60.51%\n",
    "0.1 lambda, AUC = 60.51%\n",
    "1.0 lambda, AUC = 60.51%\n",
    "10.0 lambda, AUC = 60.51%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Cross Validation #\n",
    "\n",
    "** Step 1: Split the data into 60% training and 40% testing set **\n",
    "\n",
    "`scala> val trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)`\n",
    "\n",
    "`scala> val train = trainTestSplit(0)`\n",
    "\n",
    "`scala> val test = trainTestSplit(1)`\n",
    "\n",
    "** Step 2: Train Logistic Regression model on training data and make predictions on the test set **\n",
    "\n",
    "`scala> val regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\n",
    "     | val model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "     | createMetrics(s\"$param L2 regularization parameter\", test, model)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> regResultsTest.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\") }\n",
    "0.0 L2 regularization parameter, AUC = 66.126842%\n",
    "0.001 L2 regularization parameter, AUC = 66.126842%\n",
    "0.0025 L2 regularization parameter, AUC = 66.126842%\n",
    "0.005 L2 regularization parameter, AUC = 66.126842%\n",
    "0.01 L2 regularization parameter, AUC = 66.093195%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3(for evaluation purposes): Train model on training data and make predictions on the training set **\n",
    "\n",
    "`scala> val regResultsTrain = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\n",
    "     | val model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "     | createMetrics(s\"$param L2 regularization parameter\", train, model)\n",
    "     | }`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scala> regResultsTrain.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\") }\n",
    "0.0 L2 regularization parameter, AUC = 66.233459%\n",
    "0.001 L2 regularization parameter, AUC = 66.233459%\n",
    "0.0025 L2 regularization parameter, AUC = 66.233459%\n",
    "0.005 L2 regularization parameter, AUC = 66.257100%\n",
    "0.01 L2 regularization parameter, AUC = 66.278745%`\n",
    "\n",
    "As we can see, our AUC increases when test on the data the model has already seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Conclusion #\n",
    "\n",
    "Thus our logistic regression model can successfuly predict whether a page is `ephemeral` or `evergreen` with good efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
