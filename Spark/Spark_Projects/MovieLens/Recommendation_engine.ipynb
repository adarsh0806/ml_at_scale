{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation models # \n",
    "\n",
    "There are two types:\n",
    "\n",
    "* **Content based filtering**: This uses the the attributes of an item to generate items similar to given item. These attributes could be text based like titles, names, tags, metadata or audio and video metadata based. User recommendations can be generated based on attributes of user profiles.\n",
    "\n",
    "* **Collaborative filtering**: This uses the wisdom of crowd approach; if two users exhibited similar preferences then we assume they are similar to each other in terms of taste. This is also called the nearest-neighbor model/user and item based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Matrix Factorization(Content Based filtering) ##\n",
    "\n",
    "MF aims to directly model the user-item matrix by representing it as a product of two smaller matrices of lower dimension.\n",
    "\n",
    "We call this 'Explicit' as we are using information given by the users themselves voluntarily like movie ratings, genre preferences etc.\n",
    "\n",
    "User/Item   Superman Spiderman Batman\n",
    "\n",
    "\n",
    " Bill--------> 4 ---------> 2 ------> x     \n",
    " \n",
    " Xhang------> x ---------> 3 ------> 4\n",
    " \n",
    " Roy---------> 2 ---------> x ------> 4\n",
    "\n",
    "\n",
    "\n",
    "If we have a sparse User-Item matrix `U x I`, our aim here is to create smaller matrices of the form `U x k` and `I x k` that are dense in nature, these smaller dimension matrices are called ** Factor Matrices **. \n",
    "\n",
    "If we were to multiply the factor matrices we could reconstruct an approximate version of the original ratings matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute a predicted rating for a user and the corresponding item:\n",
    "\n",
    "* Compute the vector dot product between the **relevant row of the user-factor matrix(user's feature vector)** and the **relevant row of the item-factor matrix(items feature vector)**\n",
    "\n",
    "**Advantages of the Matrix Factorization:**\n",
    "\n",
    "* Ease of computing recommendations once the model is created.\n",
    "* Good performance\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* More complex when compared nearest neighbor filtering.\n",
    "* Computationally intensive during the models training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Matrix Factorization ##\n",
    "\n",
    "This form of MF is based on implicit feedback that is not given to us but can be inferred from the interactions the user might have with an item. \n",
    "\n",
    "For example:\n",
    "* Binary data like if a user watched a movie, bought a book.\n",
    "* Count data like how many movies watched or books bought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib implements Implicit MF in the following way:\n",
    "* ** P (binary Preference matrix)**: This informs us if the movie was viewed by the user.\n",
    "* ** C (matrix of Confidence weights)**: This tells us how many times the movie was viewed by the user.\n",
    "\n",
    "The implicit model creates a user-item factor matrix. But the aim is not tell us the rating that a user will give to a movie, the score it will predict is an estimate fo the preference of a user for a given item, generally the score will be in the range 0,1.\n",
    "\n",
    "** For example:**\n",
    "\n",
    "The Preference matrix can look like:\n",
    "\n",
    "User/Item   Superman Spiderman Batman\n",
    "\n",
    "\n",
    " Bill--------> 1 ---------> 0 ------> 0     \n",
    " \n",
    " Xhang------> 0 ---------> 0 ------> 1\n",
    " \n",
    " Roy---------> 1 ---------> 1 ------> 1\n",
    " \n",
    "Here 1 indicates that the user watched the movie.\n",
    "\n",
    "The Confidence matrix can look like:\n",
    "\n",
    "User/Item   Superman Spiderman Batman\n",
    "\n",
    "\n",
    " Bill--------> 4 ---------> 0 ------> 0     \n",
    " \n",
    " Xhang------> 0 ---------> 0 ------> 2\n",
    " \n",
    " Roy---------> 2 ---------> 3 ------> 6\n",
    " \n",
    "Here the numbers indicates the number of times the user watched that movie. We can infer that the more number of times that the user watched the movie, the more that person liked it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares ##\n",
    "\n",
    "ALS is an optimization technique to solve MF problems. \n",
    "\n",
    "It works by iteratively solving a series of least squares regression problems. At each iteration:\n",
    "\n",
    "* One of the user- or item-factor matrices is treated as fixed while the other is updated using the fixed factor and rating data. \n",
    "* Then the factor matrix that was solved for is turn, treated and fixed, while the other matrix is updated. \n",
    "* This process continues until the model has converged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Extracting the right features from the data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the explicit Matrix Factorization approach using the explicit features: userID, movieID, ratings for each user-movie pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start spark shell\n",
    "\n",
    "`$Adarshs-MacBook-Pro:spark-2.0.1-bin-hadoop2.7 adarshnair$ ./bin/spark-shell --driver-memory 4g`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the ratings dataset ##\n",
    "\n",
    "** u.data ** \n",
    "* The full u data set, 100000 ratings by 943 users on 1682 items. Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The time stamps are unix seconds since 1/1/1970 UTC. The data is randomly ordered. This is a tab separated list of `user id | item id | rating | timestamp. `\n",
    "\n",
    "`scala> val rawData = sc.textFile(\"/Users/adarshnair/spark-2.0.1-bin-hadoop2.7/spark_projects/MovieLens/ml-100k/u.data\")`\n",
    "\n",
    "`scala> rawData.first()\n",
    "res2: String = 196\t242\t3\t881250949`\n",
    "\n",
    "### Remove the timestamp data and split each record on '\\t' ###\n",
    "\n",
    "`scala> val rawRatings = rawData.map(_.split(\"\\t\").take(3))`\n",
    "\n",
    "Data now looks like:\n",
    "\n",
    "`scala> rawRatings.first()\n",
    "res3: Array[String] = Array(196, 242, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Spark MLlib Recommendation library ##\n",
    "\n",
    "We wil use the MLlib library. \n",
    "\n",
    "### Import the ALS recommendation library ###\n",
    "\n",
    "Import Spark's ALS recommendation model and inspect the train method\n",
    "\n",
    "`scala> import org.apache.spark.mllib.recommendation.ALS`\n",
    "\n",
    "\n",
    "### Import Rating library ###\n",
    "\n",
    "https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/mllib/recommendation/Rating.html\n",
    "\n",
    "Import the Rating class\n",
    "\n",
    "`scala> import org.apache.spark.mllib.recommendation.Rating`\n",
    "\n",
    "`scala> Rating()\n",
    "error: not enough arguments for method apply: (user: Int, product: Int, rating: Double)org.apache.spark.mllib.recommendation.Rating in object Rating.`\n",
    "\n",
    "We need to provide the Rating() function with three arguments:\n",
    "* user\n",
    "* product\n",
    "* rating\n",
    "\n",
    "** Construct the RDD of Rating objects around user id(user), movie id(product) and rating(rating). ** \n",
    "\n",
    "`scala> val ratings = rawRatings.map { case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble) }`\n",
    "\n",
    "We use toInt, toDouble to convert string objects to numeric.\n",
    "\n",
    "View the new ratings RDD:\n",
    "\n",
    "`scala> ratings.first()\n",
    "res5: org.apache.spark.mllib.recommendation.Rating = Rating(196,242,3.0)`\n",
    "\n",
    "\n",
    "We now have the data we need in the format we need - userid, movieid, rating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the recommendation model using explicit data##\n",
    "\n",
    "Train the ALS model using `ALS.train`. This function takes 3 parameters:\n",
    "\n",
    "* **rank**: This is the number of hidden features in our low rank approximation matrices. The higher the rank the better but it directly impacts memory usage. Ideal range: 0-200\n",
    "\n",
    "* **iterations**: The number of iterations, usually kept at around 10.\n",
    "\n",
    "* **lambda**: Controls the regularization of the model and thereby mitigates overfitting. The higher the value of lambda, the more is the regularization applied. No 'ideal' values here, best to tune it using out of sample test data and cross validation.\n",
    "\n",
    "** We will use rank = 50, iterations = 10, lambda = 0.01.**\n",
    "\n",
    "`scala> val model = ALS.train(ratings, 50, 10, 0.01)`\n",
    "\n",
    "This returns a `MatrixFactorizationModel` object which contains the user and item factors in the form of an RDD (id, factor). They can be called using `userFeatures` and `productFeatures`.\n",
    "\n",
    "`scala> model.userFeatures\n",
    "res6: org.apache.spark.rdd.RDD[(Int, Array[Double])] = users MapPartitionsRDD[211] at mapValues at ALS.scala:268`\n",
    "\n",
    "Count the number of user features:\n",
    "\n",
    "`scala> model.userFeatures.count\n",
    "res7: Long = 943`\n",
    "\n",
    "Count the product features:\n",
    "\n",
    "`scala> model.productFeatures.count\n",
    "res8: Long = 1682`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the recommendation model using implicit data##\n",
    "\n",
    "This can b doing using the `ALS.trainImplicit` method which has the same parameters as `ALS.train` and one more:\n",
    "\n",
    "* **alpha**: Baseline level of confidence weighting applied. A high value for example tends to make the model more confident about the fact missing data equates to no preference for the relevant user-item pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Using the trained model to make predictions ##\n",
    "\n",
    "There are 2 main approaches:\n",
    "\n",
    "* **user-based**: Ratings of similar users on items are used to compute recommendations for a user.\n",
    "* **item-based**: Similarity of items the user has rated is used to compute recommendations for a user.\n",
    "\n",
    "We are using the MF approach: dot product between user-factor vector and the item-factor vector.\n",
    "\n",
    "MLlib's recommendation model is based on Matrix Factorization.\n",
    "\n",
    "## Step 3.1: User-Based apprach using `MatrixFactorizationModel` ###\n",
    "\n",
    "Using the `predict()` method of `MatrixFactorizationModel` we will compute a predicted score for a given user-item combination.\n",
    "\n",
    "** Make a prediction for a single user-item pair ** \n",
    "\n",
    "`scala> val predictedRating = model.predict(789, 123)\n",
    "predictedRating: Double = 3.262303054887396`\n",
    "\n",
    "*For user 789 and movie 123, the predicted movie rating is 3.26.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the top-k recommended movies for a user ##\n",
    "\n",
    "We will do this using the `recommendProducts()` method.\n",
    "\n",
    "The top 10 recommended movies for user 841 will computed as follows:\n",
    "\n",
    "`scala> val userId = 841\n",
    "userId: Int = 841`\n",
    "\n",
    "`scala> val K = 10\n",
    "K: Int = 10`\n",
    "\n",
    "`scala> val topKRecs = model.recommendProducts(userId, K)`\n",
    "\n",
    "`scala> println(topKRecs.mkString(\"\\n\"))\n",
    "Rating(841,690,5.76429175593364)\n",
    "Rating(841,408,5.6560791568740365)\n",
    "Rating(841,606,5.583861023045172)\n",
    "Rating(841,813,5.52045983893597)\n",
    "Rating(841,699,5.512711368309603)\n",
    "Rating(841,969,5.491543598823136)\n",
    "Rating(841,89,5.105260461263954)\n",
    "Rating(841,919,5.052739029383338)\n",
    "Rating(841,313,5.022547228198433)\n",
    "Rating(841,311,5.0135961696036135)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the recommended movie names ##\n",
    "\n",
    "Load the movie data which is of the form:\n",
    "\n",
    "**u.item **\n",
    "* Information about the items (movies); this is a tab separated list of the following. The last 19 fields are the genres, a 1 indicates the movie is of that genre, a 0 indicates it is not; movies can be in several genres at once. The movie ids are the ones used in the u.data data set.\n",
    "          `movie id | movie title | release date | video release date |\n",
    "          IMDb URL | unknown | Action | Adventure | Animation |\n",
    "          Children's | Comedy | Crime | Documentary | Drama | Fantasy |\n",
    "          Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\n",
    "          Thriller | War | Western |`\n",
    "          \n",
    "\n",
    "** Save the u.item data in `movies`.**\n",
    "\n",
    "`scala> val movies = sc.textFile(\"/Users/adarshnair/spark-2.0.1-bin-hadoop2.7/spark_projects/MovieLens/ml-100k/u.item\")\n",
    "movies: org.apache.spark.rdd.RDD[String] = /Users/adarshnair/spark-2.0.1-bin-hadoop2.7/spark_projects/MovieLens/ml-100k/u.item MapPartitionsRDD[218] at textFile at <console>:26`\n",
    "\n",
    "** Get the `title` from `movies` and return it in the form `137 -> Big Night (1996)`: **\n",
    "\n",
    "`scala> val titles = movies.map(line => line.split(\"\\\\|\").take(2))\n",
    "                           .map(array => (array(0).toInt,array(1)))\n",
    "                           .collectAsMap()`\n",
    "\n",
    "** Check movie title of random movie: **\n",
    "\n",
    "\n",
    "`scala> titles(345)\n",
    "res11: String = Deconstructing Harry (1997)`\n",
    "\n",
    "** Check movies rated by user 841 **\n",
    "\n",
    "We wil do this using `keyBy` to generate an RDD of key-value pairs from ratings where the key will be User ID. We then use lookup() to find the movies that user 841 has rated.\n",
    "\n",
    "`scala> val moviesForUser = ratings.keyBy(_.user).lookup(841)`\n",
    "`moviesForUser: Seq[org.apache.spark.mllib.recommendation.Rating] = WrappedArray(Rating(841,892,3.0), Rating(841,331,5.0), Rating(841,748,4.0), Rating(841,288,3.0), Rating(841,270,4.0), Rating(841,353,1.0), Rating(841,286,5.0), Rating(841,272,4.0), Rating(841,754,4.0), Rating(841,323,3.0), Rating(841,307,5.0), Rating(841,300,4.0), Rating(841,358,1.0), Rating(841,271,4.0), Rating(841,344,3.0), Rating(841,873,4.0), Rating(841,678,4.0), Rating(841,333,4.0), Rating(841,313,5.0), Rating(841,306,4.0), Rating(841,689,5.0), Rating(841,326,4.0), Rating(841,315,4.0), Rating(841,1294,5.0), Rating(841,325,3.0), Rating(841,258,5.0), Rating(841,751,3.0), Rating(841,302,5.0), Rating(841,316,4.0), Rating(841,322,4.0), Rating(841,888,5.0))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Count the number of movies rated by user 841 ** \n",
    "\n",
    "`scala> println(moviesForUser.size)\n",
    "31`\n",
    "\n",
    "** Get the names of the top 10 movies that user 841 has rated the highest **\n",
    "\n",
    "We will use the `ratings` object which we defined as:\n",
    "\n",
    "`scala> val ratings = rawRatings.map { case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble) }`\n",
    "\n",
    "earlier, and the `moviesForUser` object which has all the movies rated by the user, sort them, take the top 10 values, map them to the format (movie, rating) and print them.\n",
    "\n",
    "`scala> moviesForUser.sortBy(-_.rating).take(10).map(rating => (titles(rating.product), rating.rating)).foreach(println)`\n",
    "\n",
    "`(Edge, The (1997),5.0)\n",
    "(English Patient, The (1996),5.0)\n",
    "(Devil's Advocate, The (1997),5.0)\n",
    "(Titanic (1997),5.0)\n",
    "(Jackal, The (1997),5.0)\n",
    "(Ayn Rand: A Sense of Life (1997),5.0)\n",
    "(Contact (1997),5.0)\n",
    "(L.A. Confidential (1997),5.0)\n",
    "(One Night Stand (1997),5.0)\n",
    "(Saint, The (1997),4.0)`\n",
    "\n",
    "** Get the names of the top 10 movies that were recommended by our algorithm to the user **\n",
    "\n",
    "We will do this using the `topKrecs` object we defined earlier which has our movie recommendations and map them to the format `(movie, rating)` and print it.\n",
    "\n",
    "\n",
    "`scala> topKRecs.map(rating => (titles(rating.product), rating.rating)).foreach(println)`\n",
    "\n",
    "`(Seven Years in Tibet (1997),5.76429175593364)\n",
    "(Close Shave, A (1995),5.6560791568740365)\n",
    "(All About Eve (1950),5.583861023045172)\n",
    "(Celluloid Closet, The (1995),5.52045983893597)\n",
    "(Little Women (1994),5.512711368309603)\n",
    "(Winnie the Pooh and the Blustery Day (1968),5.491543598823136)\n",
    "(Blade Runner (1982),5.105260461263954)\n",
    "(City of Lost Children, The (1995),5.052739029383338)\n",
    "(Titanic (1997),5.022547228198433)\n",
    "(Wings of the Dove, The (1997),5.0135961696036135)`\n",
    "\n",
    "\n",
    "Using these two lists of movies, we can make our own judgement as to whether user 841 will like our recommended list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Item based approach - finding the movies most similar to a certain movie. ##\n",
    "\n",
    "Item based similarity is computed by comparing the vector representation of two items using a similarity measure.\n",
    "\n",
    "The similariy measures are as follows:\n",
    "\n",
    "* **Pearson Correlation, Cosine similarity**: for real valued vectors\n",
    "* **Jaccard similarity**: for binary vectors\n",
    "\n",
    "We have to compare the feature vector of a chosen item with other items using a similarity metric. We have to create a feature vector object out of the feature vectors which are in the Array[Double] form. \n",
    "\n",
    "Here I had to do the following to run the jblas library:\n",
    "\n",
    "* brew install maven\n",
    "* git clone https://github.com/mikiobraun/jblas.git\n",
    "* cd jblas\n",
    "* mvn install\n",
    "\n",
    "Then, I had to relaunch scala with the --packages parameter and the location of the jblas package as follows:\n",
    "\n",
    "`Adarshs-MacBook-Pro:spark-2.0.1-bin-hadoop2.7 adarshnair$ ./bin/spark-shell --driver-memory 4g --packages org.jblas:jblas:1.2.4-SNAPSHOT`\n",
    "\n",
    "** Import the jblas library: ** \n",
    "\n",
    "`scala> import org.jblas.DoubleMatrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define a jblas.DoubleMatrix object **\n",
    "\n",
    "`scala> val aMatrix = new DoubleMatrix(Array(1.0, 2.0, 3.0))\n",
    "aMatrix: org.jblas.DoubleMatrix = [1.000000; 2.000000; 3.000000]`\n",
    "\n",
    "** Define the cosine similarity function **\n",
    "\n",
    "The cosine similarity is the measure of the angle between two vectors in an n-dimensional space. It has the following steps:\n",
    "\n",
    "* Calculate dot product between the vectors\n",
    "* Divide result by denominator\n",
    "\n",
    "Cosine similarity is the normalized dot product. \n",
    "\n",
    "`scala> def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = { vec1.dot(vec2) / (vec1.norm2() * vec2.norm2()) }\n",
    "cosineSimilarity: (vec1: org.jblas.DoubleMatrix, vec2: org.jblas.DoubleMatrix)Double`\n",
    "\n",
    "The value of the cosine similarity goes from -1 to 1 with 1 being similar and -1 being dissimilar while 0 is independent.\n",
    "\n",
    "** Apply cosine similarity to a movie **\n",
    "\n",
    "`scala> val itemId = 404\n",
    "itemId: Int = 404`\n",
    "\n",
    "** Collect itemFactor of movie using lookup()**\n",
    "\n",
    "`scala> val itemFactor = model.productFeatures.lookup(itemId).head\n",
    "itemFactor: Array[Double] = Array(1.456715703010559, 0.09094125032424927, -1.323241114616394, -0.44890183210372925, 1.232578992843628, -0.15153035521507263, 0.28330186009407043, 0.14474798738956451, -0.1086246594786644, 1.0265341997146606, -0.8233844041824341, -0.28477057814598083, 0.20904113352298737, 0.5610338449478149, -0.17029500007629395, 0.5896046161651611, 0.45187196135520935, -0.36535635590553284, -1.1037601232528687, -1.2145742177963257, 1.314957618713379, -0.4275266230106354, -0.7305580377578735, -0.49946409463882446, 0.46623849868774414, 1.051638126373291, -0.4805344343185425, 0.4812560975551605, 0.47500789165496826, -0.14856795966625214, -0.6726163029670715, -0.2977805435657501, -1.1333670616149902, -0.9176173806190491, -0.11220163106918335, 0.17123645544052124, -0.365033537...\n",
    "`\n",
    "\n",
    "** Collect itemVector of same movie **\n",
    "\n",
    "`scala> val itemVector = new DoubleMatrix(itemFactor)\n",
    "itemVector: org.jblas.DoubleMatrix = [1.456716; 0.090941; -1.323241; -0.448902; 1.232579; -0.151530; 0.283302; 0.144748; -0.108625; 1.026534; -0.823384; -0.284771; 0.209041; 0.561034; -0.170295; 0.589605; 0.451872; -0.365356; -1.103760; -1.214574; 1.314958; -0.427527; -0.730558; -0.499464; 0.466238; 1.051638; -0.480534; 0.481256; 0.475008; -0.148568; -0.672616; -0.297781; -1.133367; -0.917617; -0.112202; 0.171236; -0.365034; 0.033380; -1.045698; -1.550930; -1.280466; 0.309942; 0.901337; 0.098294; 0.651365; 0.500232; -0.596415; -0.1`\n",
    "\n",
    "** Find cosineSimilarity of movie with itself using itemFactor and itemVector **\n",
    "\n",
    "`scala> cosineSimilarity(itemVector, itemVector)\n",
    "res4: Double = 1.0`\n",
    "\n",
    "As expected we get a value of 1.\n",
    "\n",
    "\n",
    "** Apply similarity metric to all movies to find the score for all movies when compared to movie with id 404**\n",
    "\n",
    "Store the scores for the movie, score pairs in the `sims` variable.\n",
    "\n",
    "`scala> val sims = model.productFeatures.map{ case (id, factor) => \n",
    "     | val factorVector = new DoubleMatrix(factor)\n",
    "     | val sim = cosineSimilarity(factorVector, itemVector)\n",
    "     | (id, sim)\n",
    "     | }\n",
    "sims: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[222] at map at <console>:43`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find the top 10 most similar movies compared to movie with id 404.**\n",
    "\n",
    "We use `top` to compute the top-k results instead of `collect` which returns all the data to the driver.\n",
    "We use `Ordering` to sort the data (item id, similarity score) pairs in the sims RDD.\n",
    "\n",
    "`scala> val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })`\n",
    "\n",
    "`scala> val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\n",
    "sortedSims: Array[(Int, Double)] = Array((404,1.0), (99,0.9044948670945991), (501,0.896898212252714), (482,0.8843526451683614), (378,0.8750775812837056), (71,0.8683390197894713), (480,0.8651580922076123), (498,0.8644220348817375), (655,0.8609436553440514), (97,0.8601814581441996))`\n",
    "\n",
    "** Print top 10 most similar movies when compared to our chosen movie with id 404.**\n",
    "\n",
    "`scala> println(sortedSims.mkString(\"\\n\"))\n",
    "(404,1.0)\n",
    "(99,0.9044948670945991)\n",
    "(501,0.896898212252714)\n",
    "(482,0.8843526451683614)\n",
    "(378,0.8750775812837056)\n",
    "(71,0.8683390197894713)\n",
    "(480,0.8651580922076123)\n",
    "(498,0.8644220348817375)\n",
    "(655,0.8609436553440514)\n",
    "(97,0.8601814581441996)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Check the name of our movie ** \n",
    "\n",
    "`scala> println(titles(itemId))\n",
    "Pinocchio (1940)`\n",
    "\n",
    "** Get the 10 movies most similar to Pinocchio in a sorted list**\n",
    "\n",
    "`scala> val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\n",
    "sortedSims2: Array[(Int, Double)] = Array((404,1.0), (99,0.9044948670945991), (501,0.896898212252714), (482,0.8843526451683614), (378,0.8750775812837056), (71,0.8683390197894713), (480,0.8651580922076123), (498,0.8644220348817375), (655,0.8609436553440514), (97,0.8601814581441996), (484,0.8601798273270549))`\n",
    "\n",
    "** Get the names of the 10 movies most similar to Pinocchio **\n",
    "\n",
    "`scala> sortedSims2.slice(1, 11).map{ case (id, sim) => (titles(id), sim) }.mkString(\"\\n\")\n",
    "res7: String =\n",
    "(Snow White and the Seven Dwarfs (1937),0.9044948670945991)\n",
    "(Dumbo (1941),0.896898212252714)\n",
    "(Some Like It Hot (1959),0.8843526451683614)\n",
    "(Miracle on 34th Street (1994),0.8750775812837056)\n",
    "(Lion King, The (1994),0.8683390197894713)\n",
    "(North by Northwest (1959),0.8651580922076123)\n",
    "(African Queen, The (1951),0.8644220348817375)\n",
    "(Stand by Me (1986),0.8609436553440514)\n",
    "(Dances with Wolves (1990),0.8601814581441996)\n",
    "(Maltese Falcon, The (1941),0.8601798273270549)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate performance of recommendation engine #\n",
    "\n",
    "We use 2 metrics:\n",
    "\n",
    "* **MSE, RMSE**\n",
    "* **Mean Average Precision at K**: The APK score will be higher if the result documents are both relevant and the relevant documents are presented higher in the results. This metric is better suited for recommender systems than MSE and RMSE. In APK, each user is the equivalent of a query, the set of top-k items is the document result set. The relevant documents are the set of items that the user interacted with. Hence APK measures how good our model is at predicting items that a user will find relevant and will choose to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1 MSE, RMSE ##\n",
    "\n",
    "### Check actual rating given by user 841 for a movie ###\n",
    "\n",
    "** Load all the movies rated by user 841 into `moviesForUser` ** \n",
    "\n",
    "`scala> val moviesForUser = ratings.keyBy(_.user).lookup(841)\n",
    "moviesForUser: Seq[org.apache.spark.mllib.recommendation.Rating] = WrappedArray(Rating(841,892,3.0), Rating(841,331,5.0), Rating(841,748,4.0), Rating(841,288,3.0), Rating(841,270,4.0), Rating(841,353,1.0), Rating(841,286,5.0), Rating(841,272,4.0), Rating(841,754,4.0), Rating(841,323,3.0), Rating(841,307,5.0), Rating(841,300,4.0), Rating(841,358,1.0), Rating(841,271,4.0), Rating(841,344,3.0), Rating(841,873,4.0), Rating(841,678,4.0), Rating(841,333,4.0), Rating(841,313,5.0), Rating(841,306,4.0), Rating(841,689,5.0), Rating(841,326,4.0), Rating(841,315,4.0), Rating(841,1294,5.0), Rating(841,325,3.0), Rating(841,258,5.0), Rating(841,751,3.0), Rating(841,302,5.0), Rating(841,316,4.0), Rating(841,322,4.0), Rating(841,888,5.0))`\n",
    "\n",
    "** Check rating by user for the first movie the user rated **\n",
    "\n",
    "`scala> val actualRating = moviesForUser.take(1)(0)\n",
    "actualRating: org.apache.spark.mllib.recommendation.Rating = Rating(841,892,3.0)`\n",
    "\n",
    "The user gave a rating of 3.0\n",
    "\n",
    "** Check predicted rating for that movie by same user ** \n",
    "\n",
    "`scala> val predictedRating = model.predict(841, actualRating.product)\n",
    "predictedRating: Double = 2.9593273900130113`\n",
    "\n",
    "### Computing MSE, RMSE across entire dataset ###\n",
    "\n",
    "We need the squared error of each (user, movie, actualRating, predictedRating) entry, sum them up and then divide by total number of ratings. \n",
    "\n",
    "** Extract user and product IDs from `ratings` RDD **\n",
    "\n",
    "`scala> val usersProducts = ratings.map{ case Rating(user, product, rating)  => (user, product)}\n",
    "usersProducts: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[228] at map at <console>:34`\n",
    "\n",
    "** Make predictions for each user-item pair using `model.predict()`: **\n",
    "\n",
    "`scala> val predictions = model.predict(usersProducts).map{ case Rating(user, product, rating) => ((user, product), rating) }\n",
    "predictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[237] at map at <console>:38`\n",
    "\n",
    "** Extract actualRating and map the `ratings` RDD so that the user-item pair is the key and the actualRating is the value. We will get two RDDs with the same form of key, so we join them together to create a new RDD with the actual and predicted ratings for each user-item combination **\n",
    "\n",
    "`scala> val ratingsAndPredictions = ratings.map{case Rating(user, product, rating) => ((user, product), rating)}.join(predictions)\n",
    "ratingsAndPredictions: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[241] at join at <console>:40`\n",
    "\n",
    "\n",
    "** Formatted code for clarity **\n",
    "\n",
    "\n",
    "`val usersProducts = ratings.map{ case Rating(user, product, rating)  => (user, product)}`\n",
    "\n",
    "*usersProducts gives (user, product). This is fed into model.predict() so predictions can be made on all movies. The user-item is the key and rating is the value.*\n",
    "\n",
    "\n",
    "`val predictions = model.predict(usersProducts)\n",
    "                        .map{ case Rating(user, product, rating) => ((user, product), rating)}`\n",
    "\n",
    "*predictions is joined with the actual ratings because both have user-item as the key.*\n",
    "\n",
    "`val ratingsAndPredictions = ratings.map{ case Rating(user, product, rating) => ((user, product), rating)}\n",
    "                                   .join(predictions)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compute MSE ** \n",
    "\n",
    "`scala> val predictedAndTrue = ratingsAndPredictions.map { case ((user, product), (actual, predicted)) => (actual, predicted) }\n",
    "predictedAndTrue: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[242] at map at <console>:42`\n",
    "\n",
    "`scala> val regressionMetrics = new RegressionMetrics(predictedAndTrue)\n",
    "regressionMetrics: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@6a45febd`\n",
    "\n",
    "`scala> println(\"Mean Squared Error = \" + regressionMetrics.meanSquaredError)\n",
    "Mean Squared Error = 0.08473321421633566`\n",
    "\n",
    "** Compute RMSE **\n",
    "\n",
    "This is equivalent to the standard deviation of the differences between the predicted and actual ratings.\n",
    "\n",
    "`scala> println(\"Root Mean Squared Error = \" + regressionMetrics.rootMeanSquaredError)\n",
    "Root Mean Squared Error = 0.2910897013230383`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2 Computing Mean Average Precision at K ##\n",
    "\n",
    "** Load the library ** \n",
    "\n",
    "`scala> import org.apache.spark.mllib.evaluation.RankingMetrics\n",
    "import org.apache.spark.mllib.evaluation.RankingMetrics`\n",
    "\n",
    "** Get all the movie IDs rated by user 841** \n",
    "\n",
    "`scala> val actualMovies = moviesForUser.map(_.product)\n",
    "actualMovies: Seq[Int] = ArrayBuffer(892, 331, 748, 288, 270, 353, 286, 272, 754, 323, 307, 300, 358, 271, 344, 873, 678, 333, 313, 306, 689, 326, 315, 1294, 325, 258, 751, 302, 316, 322, 888)`\n",
    "\n",
    "** Use the top 10 recommended movies to compute the APK score**\n",
    "\n",
    "`scala> val predictedMovies = topKRecs.map(_.product)\n",
    "predictedMovies: Array[Int] = Array(185, 23, 187, 180, 175, 56, 127, 276, 443, 89)`\n",
    "\n",
    "We need to compute the APK for each user and average them out to find the MAPK.\n",
    "\n",
    "### Compute recommendations for all users ###\n",
    "\n",
    "** Get the itemFactors and itemMatrix **\n",
    "\n",
    "`scala> val itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect()\n",
    "itemFactors: Array[Array[Double]] = Array(Array(0.9928126931190491, 0.5980018377304077, -0.9922769069671631, -0.5807792544364929, 0.6617379784584045, -0.6692140102386475, 0.29881414771080017, -0.6581135392189026, -0.51359623670578, 0.4164090156555176, -0.4938781261444092, 0.21485476195812225, -0.30804479122161865, 1.064658284187317, 0.2200748324394226, 1.1584419012069702, 0.21837905049324036, 0.04997337982058525, -1.7830859422683716, -0.7733256816864014, 1.1847765445709229, -0.21797437965869904, -0.6154626607894897, -0.6413697600364685, -0.5309175848960876, 0.7262206077575684, 0.03381478786468506, 1.2232928276062012, -0.16844823956489563, -0.14984336495399475, -0.36415931582450867, -0.2926683723926544, -1.5741336345672607, -1.30470609664917, 0.11398006975650787, -0.5297638773918152, -0....`\n",
    "\n",
    "The itemMatrix is the DoubleMatrix of itemFactors.\n",
    "\n",
    "`scala> val itemMatrix = new DoubleMatrix(itemFactors)\n",
    "itemMatrix: org.jblas.DoubleMatrix = [0.992813, 0.598002, -0.992277, -0.580779, 0.661738, -0.669214, 0.298814, -0.658114, -0.513596, 0.416409, -0.493878, 0.214855, -0.308045, 1.064658, 0.220075, 1.158442, 0.218379, 0.049973, -1.783086, -0.773326, 1.184777, -0.217974, -0.615463, -0.641370, -0.530918, 0.726221, 0.033815, 1.223293, -0.168448, -0.149843, -0.364159, -0.292668, -1.574134, -1.304706, 0.113980, -0.529764, -0.930787, -0.020343, -0.519978, -0.950019, -1.123361, 0.610111, 1.118598, 0.011164, 0.602885, 0.326406, -0.408680, -0.165954, -0.687462, 0.268771; 1.297704, -0.220837, -0.881212, -1.140893, 0.670369, -0.623236, 0.553994, 0.824082, 0.162887, 0.677236, -0.329635, 0.008709, 0.466816, 1.550000, -0.548380, 1.163319, -0.977304, 0.286353, -0.828565, -0.754250, 1.354773, 0.029863, -0...`\n",
    "\n",
    "\n",
    "We get the itemMatric with the following dimensions.\n",
    "\n",
    "`scala> println(itemMatrix.rows, itemMatrix.columns)\n",
    "(1682,50)`\n",
    "\n",
    "There are 1682 total movies(rows) and a factor dimension of 50(columns).\n",
    "\n",
    "** Broadcast the item matrix to all worker nodes **\n",
    "\n",
    "`scala> val imBroadcast = sc.broadcast(itemMatrix)\n",
    "imBroadcast: org.apache.spark.broadcast.Broadcast[org.jblas.DoubleMatrix] = Broadcast(54)`\n",
    "\n",
    "** Compute recommendations for each user **\n",
    "\n",
    "* Apply map() to each user factor within which we perform a matrix multiplication between the user-factor vector and the movie-factor vector.\n",
    "* The result is a vector of length 1682(the total number of movies) with the predicted rating for each movie.\n",
    "\n",
    "`scala> val allRecs = model.userFeatures.map{ case (userId, array) => \n",
    "     | val userVector = new DoubleMatrix(array)\n",
    "     | val scores = imBroadcast.value.mmul(userVector)\n",
    "     | val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)\n",
    "     | val recommendedIds = sortedWithId.map(_._2 + 1).toSeq\n",
    "     | (userId, recommendedIds)\n",
    "     | }\n",
    "allRecs: org.apache.spark.rdd.RDD[(Int, Seq[Int])] = MapPartitionsRDD[245] at map at <console>:45`\n",
    "\n",
    "We now have an RDD that contains the list of movie IDs for each user ID. The movie IDs are sorted in order of predicted rating.\n",
    "\n",
    "** Get all the movie ids per user, grouped by user id**\n",
    "\n",
    "`scala> val userMovies = ratings.map{ case Rating(user, product, rating) => (user, product) }.groupBy(_._1)\n",
    "userMovies: org.apache.spark.rdd.RDD[(Int, Iterable[(Int, Int)])] = ShuffledRDD[248] at groupBy at <console>:35`\n",
    "\n",
    "** Join the two RDDs on the user ID key. For each user we have the list of actual and predicted movie IDs that we pass into our MAPK error metric **\n",
    "\n",
    "`scala> val predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
    "     | val actual = actualWithIds.map(_._2)\n",
    "     | (predicted.toArray, actual.toArray)\n",
    "     | }\n",
    "predictedAndTrueForRanking: org.apache.spark.rdd.RDD[(Array[Int], Array[Int])] = MapPartitionsRDD[252] at map at <console>:49`\n",
    "\n",
    "Give predictedAndTrueForRanking as the argument to the RankingMetrics().\n",
    "\n",
    "`scala> val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)\n",
    "rankingMetrics: org.apache.spark.mllib.evaluation.RankingMetrics[Int] = org.apache.spark.mllib.evaluation.RankingMetrics@80e095`\n",
    "\n",
    "** Compute MAP **\n",
    "\n",
    "`scala> println(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision)\n",
    "Mean Average Precision = 0.07284042040123272`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "\n",
    "Our recommendation engine performs with the following performance:\n",
    "\n",
    "* MSE: 0.08\n",
    "* RMSE: 0.29\n",
    "* MAP: 0.0728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
