{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Working with HDFS using cmd #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 42241163264 (39.34 GB)\n",
      "Present Capacity: 37587231919 (35.01 GB)\n",
      "DFS Remaining: 37397528576 (34.83 GB)\n",
      "DFS Used: 189703343 (180.92 MB)\n",
      "DFS Used%: 0.50%\n",
      "Under replicated blocks: 0\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:50010 (localhost)\n",
      "Hostname: sparkbox\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 42241163264 (39.34 GB)\n",
      "DFS Used: 189703343 (180.92 MB)\n",
      "Non DFS Used: 4653931345 (4.33 GB)\n",
      "DFS Remaining: 37397528576 (34.83 GB)\n",
      "DFS Used%: 0.45%\n",
      "DFS Remaining%: 88.53%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Oct 06 16:40:53 UTC 2016\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - vagrant supergroup          0 2016-05-11 19:52 /spark\r\n",
      "drwxr-xr-x   - vagrant supergroup          0 2016-10-06 16:40 /user\r\n"
     ]
    }
   ],
   "source": [
    "# list the content of the root directory\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem               Size     Used  Available  Use%\r\n",
      "hdfs://localhost:9000  39.3 G  180.9 M     34.8 G    0%\r\n"
     ]
    }
   ],
   "source": [
    "# -df: amount of available disk space in HDFS\n",
    "!hdfs dfs -df -h /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.0 M  /spark\r\n",
      "473.4 K  /user\r\n"
     ]
    }
   ],
   "source": [
    "# -du: size of each folder (du stands for disk usage)\n",
    "!hdfs dfs -du -h /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a datasets directory\n",
    "!hdfs dfs -mkdir /datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transfer files from the hard disk to the node of the DFS\n",
    "!wget -q http://www.gutenberg.org/cache/epub/100/pg100.txt \\\n",
    "    -O ../datasets/shakespeare_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the data in the datasets directory that we created\n",
    "!hdfs dfs -put ../datasets/shakespeare_all.txt \\\n",
    "    /datasets/shakespeare_all.txt\n",
    "\n",
    "!hdfs dfs -put ../datasets/hadoop_git_readme.txt \\\n",
    "    /datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 vagrant supergroup       1365 2016-10-06 16:58 /datasets/hadoop_git_readme.txt\r\n",
      "-rw-r--r--   1 vagrant supergroup    5589889 2016-10-06 16:58 /datasets/shakespeare_all.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\r\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the files to the standard output\n",
    "# count the number of newlines \n",
    "!hdfs dfs -cat /datasets/hadoop_git_readme.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\r\n"
     ]
    }
   ],
   "source": [
    "# Concatenate files that are on HDFS and our local file system\n",
    "!hdfs dfs -cat \\\n",
    "    hdfs:///datasets/hadoop_git_readme.txt \\\n",
    "    file:///home/vagrant/datasets/hadoop_git_readme.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy in HDFS using -cp\n",
    "!hdfs dfs -cp /datasets/hadoop_git_readme.txt \\\n",
    "    /datasets/copy_hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/10/06 17:18:58 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /datasets/copy_hadoop_git_readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Remove a a file using -rm\n",
    "!hdfs dfs -rm /datasets/copy_hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/10/06 17:19:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n"
     ]
    }
   ],
   "source": [
    "# Empty thrashed data using -expunge\n",
    "!hdfs dfs -expunge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a file from HDFS to the local machine\n",
    "!hdfs dfs -get /datasets/hadoop_git_readme.txt \\\n",
    "    /tmp/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntry, of \r\n",
      "encryption software.  BEFORE using any encryption software, please \r\n",
      "check your country's laws, regulations and policies concerning the\r\n",
      "import, possession, or use, and re-export of encryption software, to \r\n",
      "see if this is permitted.  See <http://www.wassenaar.org/> for more\r\n",
      "information.\r\n",
      "\r\n",
      "The U.S. Government Department of Commerce, Bureau of Industry and\r\n",
      "Security (BIS), has classified this software as Export Commodity \r\n",
      "Control Number (ECCN) 5D002.C.1, which includes information security\r\n",
      "software using or performing cryptographic functions with asymmetric\r\n",
      "algorithms.  The form and manner of this Apache Software Foundation\r\n",
      "distribution makes it eligible for export under the License Exception\r\n",
      "ENC Technology Software Unrestricted (TSU) exception (see the BIS \r\n",
      "Export Administration Regulations, Section 740.13) for both object \r\n",
      "code and source code.\r\n",
      "\r\n",
      "The following provides more details on the included cryptographic\r\n",
      "software:\r\n",
      "  Hadoop Core uses the SSL libraries from the Jetty project written \r\n",
      "by mortbay.org."
     ]
    }
   ],
   "source": [
    "# Display the last KB of data using -tail\n",
    "!hdfs dfs -tail /datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Working with HDFS using Snakebite #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install snakebite\n",
    "from snakebite.client import Client\n",
    "# Instantiate client and define the port of the Name node(9000)\n",
    "# Name node - master node which contains the metadata of the files in the distributed filesystem and doesnt store any\n",
    "# actual data. Data node - store blocks of data in chunks of 64MB typically.\n",
    "client = Client(\"localhost\", 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blockSize': 134217728L,\n",
       " 'bytesPerChecksum': 512,\n",
       " 'checksumType': 2,\n",
       " 'encryptDataTransfer': False,\n",
       " 'fileBufferSize': 4096,\n",
       " 'replication': 1,\n",
       " 'trashInterval': 0L,\n",
       " 'writePacketSize': 65536}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get generic information\n",
    "client.serverdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets\n",
      "/spark\n",
      "/user\n"
     ]
    }
   ],
   "source": [
    "# List the files and directories in root\n",
    "for x in client.ls(['/']):\n",
    "    print x['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capacity': 42241163264L,\n",
       " 'corrupt_blocks': 0L,\n",
       " 'filesystem': 'hdfs://localhost:9000',\n",
       " 'missing_blocks': 0L,\n",
       " 'remaining': 37384859648L,\n",
       " 'under_replicated': 0L,\n",
       " 'used': 195366912L}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df(disk free)\n",
    "client.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'length': 5591254L, 'path': '/datasets'},\n",
       " {'length': 187698038L, 'path': '/spark'},\n",
       " {'length': 484810L, 'path': '/user'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# du(disk usage)\n",
    "list(client.du([\"/\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/datasets/shakespeare_all.txt', 'result': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete a file from HDFS\n",
    "# delete command returns a generator and the execution never fails, the result of the operation is in the result field.\n",
    "client.delete(['/datasets/shakespeare_all.txt']).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': '',\n",
       " 'path': '/tmp/hadoop_git_readme_2.txt',\n",
       " 'result': True,\n",
       " 'source_path': '/datasets/hadoop_git_readme.txt'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy a file from HDFS to the local filesystem. Output is a generator, therefore we need to check the output dict\n",
    "# to see for success\n",
    "(client\n",
    ".copyToLocal(['/datasets/hadoop_git_readme.txt'], \n",
    "             '/tmp/hadoop_git_readme_2.txt')\n",
    ".next())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '/datasets_2', 'result': True}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a directory \n",
    "list(client.mkdir(['/datasets_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '/datasets', 'result': True},\n",
       " {'path': '/datasets_2', 'result': True}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete all files matching a string\n",
    "list(client.delete(['/datasets*'], recurse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
