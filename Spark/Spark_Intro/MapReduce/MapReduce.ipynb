{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Running a Hadoop job using only Python code. #\n",
    "\n",
    "# Step 1: Load the dataset containing all of Shakespeares bibliography #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 vagrant supergroup       1365 2016-10-06 18:40 /datasets/hadoop_git_readme.txt\r\n",
      "-rw-r--r--   1 vagrant supergroup    5589889 2016-10-06 18:40 /datasets/shakespeare_all.txt\r\n"
     ]
    }
   ],
   "source": [
    "# Make a directory\n",
    "!hdfs dfs -mkdir -p /datasets\n",
    "\n",
    "# Download the datasets\n",
    "!wget -q http://www.gutenberg.org/cache/epub/100/pg100.txt \\\n",
    "    -O ../datasets/shakespeare_all.txt\n",
    "    \n",
    "# Get the texts\n",
    "!hdfs dfs -put -f ../datasets/shakespeare_all.txt /datasets/shakespeare_all.txt\n",
    "!hdfs dfs -put -f ../datasets/hadoop_git_readme.txt /datasets/hadoop_git_readme.txt\n",
    "\n",
    "# List the datasets downloaded\n",
    "!hdfs dfs -ls /datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create the mapper and the reducer #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Mapper\n",
    "\n",
    "The mapper reads lines from the stdin and prints the key:value pairs of the number of characters(except the newline), the number of words(splitting the line onthe whitespace), the number of lines(always 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapper\n",
    "with open('mapper_hadoop.py', 'w') as fh:\n",
    "    fh.write(\"\"\"#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print \"chars\", len(line.rstrip('\\\\n'))\n",
    "    print \"words\", len(line.split())\n",
    "    print \"lines\", 1\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2 Reducer ##\n",
    "\n",
    "The reducer sums up the values for each key and prints the grand total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reducer\n",
    "with open('reducer_hadoop.py', 'w') as fh:\n",
    "    fh.write(\"\"\"#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "counts = {\"chars\": 0, \"words\":0, \"lines\":0}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    kv = line.rstrip().split()\n",
    "    counts[kv[0]] += int(kv[1])\n",
    "\n",
    "for k,v in counts.items():\n",
    "    print k, v\n",
    "    \"\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run MapReduce #\n",
    "\n",
    "## Step 3.1: Running on local system ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x *_hadoop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars 1335\r\n",
      "lines 31\r\n",
      "words 179\r\n"
     ]
    }
   ],
   "source": [
    "# Shuffler is replaced with sort -k1,1 which sorts the input strings using the first field(key)\n",
    "# Piping, concatenate the dataset -> mapper -> shuffler -> reducer\n",
    "\n",
    "!cat ../datasets/hadoop_git_readme.txt | ./mapper_hadoop.py | sort -k1,1 | ./reducer_hadoop.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Running on Hadoop ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dir to store results\n",
    "!hdfs dfs -mkdir -p /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar8239820158064390153/] [] /tmp/streamjob3508329584284935653.jar tmpDir=null\n",
      "16/10/06 18:54:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/10/06 18:54:33 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/10/06 18:54:33 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/10/06 18:54:33 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/10/06 18:54:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1475771633082_0004\n",
      "16/10/06 18:54:34 INFO impl.YarnClientImpl: Submitted application application_1475771633082_0004\n",
      "16/10/06 18:54:34 INFO mapreduce.Job: The url to track the job: http://sparkbox:8088/proxy/application_1475771633082_0004/\n",
      "16/10/06 18:54:34 INFO mapreduce.Job: Running job: job_1475771633082_0004\n",
      "16/10/06 18:54:41 INFO mapreduce.Job: Job job_1475771633082_0004 running in uber mode : false\n",
      "16/10/06 18:54:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/10/06 18:54:47 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/10/06 18:54:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/10/06 18:54:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/10/06 18:54:58 INFO mapreduce.Job: Job job_1475771633082_0004 completed successfully\n",
      "16/10/06 18:54:59 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1060\n",
      "\t\tFILE: Number of bytes written=332854\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2256\n",
      "\t\tHDFS: Number of bytes written=33\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8309\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3994\n",
      "\t\tTotal time spent by all map tasks (ms)=8309\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3994\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8309\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3994\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8508416\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4089856\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31\n",
      "\t\tMap output records=93\n",
      "\t\tMap output bytes=868\n",
      "\t\tMap output materialized bytes=1066\n",
      "\t\tInput split bytes=208\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=23\n",
      "\t\tReduce shuffle bytes=1066\n",
      "\t\tReduce input records=93\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=186\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=88\n",
      "\t\tCPU time spent (ms)=2090\n",
      "\t\tPhysical memory (bytes) snapshot=672653312\n",
      "\t\tVirtual memory (bytes) snapshot=2492366848\n",
      "\t\tTotal committed heap usage (bytes)=513277952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2048\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=33\n",
      "16/10/06 18:54:59 INFO streaming.StreamJob: Output directory: /tmp/mr.out\n"
     ]
    }
   ],
   "source": [
    "# Remove anything inside the tmp folder\n",
    "!hdfs dfs -rm -f -r /tmp/mr.out\n",
    "\n",
    "'''\n",
    "4 step process:\n",
    "\n",
    "1. We want to use the Hadoop streaming capability.\n",
    "2. Distribute the files to each mapper as they are local files (-files)\n",
    "3. Set the mapper and reducer we are using. (-mapper, -reducer)\n",
    "4. Define the input file (-input) and the output director y (-output)\n",
    "'''\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar \\\n",
    "-files mapper_hadoop.py,reducer_hadoop.py \\\n",
    "-mapper mapper_hadoop.py -reducer reducer_hadoop.py \\\n",
    "-input /datasets/hadoop_git_readme.txt -output /tmp/mr.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Studying the output # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 vagrant supergroup          0 2016-10-06 18:54 /tmp/mr.out/_SUCCESS\r\n",
      "-rw-r--r--   1 vagrant supergroup         33 2016-10-06 18:54 /tmp/mr.out/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/mr.out\n",
    "'''\n",
    "There are 2 files:\n",
    "1. _SUCCESS and indicates that the MR job has finished the writing stage in the directory.\n",
    "2. part-00000 which contains the actual results\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Using the MrJob library to run the Hadoop job. #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Write the python file using the MrJob functionalities #\n",
    "\n",
    "Mappers and reducers are wrapepd in a subclass of MRJob. Inputs are not read from stdin, but passed as a function argument and outputs are not printed, but yielded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"MrJob_job1.py\", \"w\") as fh:\n",
    "    fh.write(\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()    \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Running it locally ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\r\n",
      "Creating temp directory /tmp/MrJob_job1.vagrant.20161006.190646.064219\r\n",
      "Running step 1 of 1...\r\n",
      "Streaming final output from /tmp/MrJob_job1.vagrant.20161006.190646.064219/output...\r\n",
      "\"chars\"\t1335\r\n",
      "\"lines\"\t31\r\n",
      "\"words\"\t179\r\n",
      "Removing temp directory /tmp/MrJob_job1.vagrant.20161006.190646.064219...\r\n"
     ]
    }
   ],
   "source": [
    "# Executes mapper and reducer, prints the result and cleans up the temporary directory\n",
    "!python MrJob_job1.py ../datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Running on Hadoop MapReduce and HDFS##\n",
    "\n",
    "We run the same python file, with the `-r hadoop` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/MrJob_job1.vagrant.20161006.190837.776982\n",
      "Using Hadoop version 2.6.4\n",
      "Copying local files to hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20161006.190837.776982/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar5878441889199379327/] [] /tmp/streamjob7184503074016446018.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1475771633082_0005\n",
      "  Submitted application application_1475771633082_0005\n",
      "  The url to track the job: http://sparkbox:8088/proxy/application_1475771633082_0005/\n",
      "  Running job: job_1475771633082_0005\n",
      "  Job job_1475771633082_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1475771633082_0005 completed successfully\n",
      "  Output directory: hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20161006.190837.776982/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2048\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1153\n",
      "\t\tFILE: Number of bytes written=337717\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2256\n",
      "\t\tHDFS: Number of bytes written=36\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6585344\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3444736\n",
      "\t\tTotal time spent by all map tasks (ms)=6431\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6431\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3364\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3364\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6431\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3364\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1880\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=78\n",
      "\t\tInput split bytes=208\n",
      "\t\tMap input records=31\n",
      "\t\tMap output bytes=961\n",
      "\t\tMap output materialized bytes=1159\n",
      "\t\tMap output records=93\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=649461760\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=93\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=1159\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=186\n",
      "\t\tTotal committed heap usage (bytes)=512753664\n",
      "\t\tVirtual memory (bytes) snapshot=2494242816\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20161006.190837.776982/output...\n",
      "\"chars\"\t1335\n",
      "\"lines\"\t31\n",
      "\"words\"\t179\n",
      "Removing HDFS temp directory hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20161006.190837.776982...\n",
      "Removing temp directory /tmp/MrJob_job1.vagrant.20161006.190837.776982...\n"
     ]
    }
   ],
   "source": [
    "!python MrJob_job1.py -r hadoop hdfs:///datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Running a process that needs more than one MapReduce step #\n",
    "\n",
    "Our objective here is to find the most common word used by Shakespere. Using MrJob we will create a cascade of MapReduce operations where each output is the input of the next stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Define the MapReducer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Stage 1:\n",
    "\n",
    "Step 1: Map (mapper_get_words)\n",
    "A key-map tuple is yielded for each word. The key is the lowercased word and the value is always 1.\n",
    "\n",
    "Step 2: reduce (reducer_count_words)\n",
    "For each key(lowercased word), we sum all the values. The o/p will tell us how many times the word appears in the text.\n",
    "\n",
    "Stage 2:\n",
    "\n",
    "Step 1: Map (mapper_word_count_one_key)\n",
    "We flip the key-value tuples and put them as values of a new key pair. To force one reducer to have all the tuples we \n",
    "assign the same key, None to each output tuple.\n",
    "\n",
    "Step 2: Reduce (reducer_find_max_word)\n",
    "We discard the only key available and extract the maximum of the values. \n",
    "'''\n",
    "\n",
    "with open(\"MrJob_job2.py\", \"w\") as fh:\n",
    "    fh.write(\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(mapper=self.mapper_word_count_one_key,\n",
    "                   reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        yield (word, sum(counts))\n",
    "    \n",
    "    def mapper_word_count_one_key(self, word, counts):\n",
    "        # send all the tuples to same reducer\n",
    "        yield None, (counts, word)\n",
    "\n",
    "    def reducer_find_max_word(self, _, count_word_pairs):\n",
    "        # each item of word_count_pairs is a tuple (count, word),\n",
    "        yield max(count_word_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Run the MapReducer #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27801\t\"the\"\r\n"
     ]
    }
   ],
   "source": [
    "# Run locally\n",
    "!python MrJob_job2.py --quiet ../datasets/shakespeare_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27801\t\"the\"\r\n"
     ]
    }
   ],
   "source": [
    "# Run on the Hadoop cluster\n",
    "!python MrJob_job2.py -r hadoop --quiet hdfs:///datasets/shakespeare_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common word used by Shakespere is 'The', used over 27k times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
