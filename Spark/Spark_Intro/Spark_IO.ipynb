{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sharing data within a cluster #\n",
    "\n",
    "## Broadcast: read-only variables ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example: let's encode the gender found in the demographic data\n",
    "# as a hot encode. The association should be the same\n",
    "# on every machine in the cluster, requiring a shared mapping\n",
    "\n",
    "one_hot_encoding = {\"M\": (1, 0, 0),\n",
    "                    \"F\": (0, 1, 0),\n",
    "                    \"U\": (0, 0, 1)\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gender one-hot-encoding using Sparck context(sc)\n",
    "(sc.parallelize([\"M\", \"F\", \"U\", \"F\", \"M\", \"U\"])\n",
    "   .map(lambda x: one_hot_encoding[x])\n",
    "   .collect())\n",
    "\n",
    "'''\n",
    "The command above works only in the single node configuration since the variable \"one_hot_encoding\" is defined only on\n",
    "this machine.\n",
    "On a multi-node cluster, it will raise an error.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution 1: include the encoding map in the .map() function \n",
    "# In this way, all the nodes will see it\n",
    "\n",
    "def map_ohe(x):\n",
    "    ohe = {\"M\": (1, 0, 0),\n",
    "           \"F\": (0, 1, 0),\n",
    "           \"U\": (0, 0, 1)\n",
    "          }\n",
    "    return ohe[x]\n",
    "\n",
    "sc.parallelize([\"M\", \"F\", \"U\", \"F\", \"M\", \"U\"]) \\\n",
    "  .map(map_ohe) \\\n",
    "  .collect()\n",
    "\n",
    "'''\n",
    "This solution will work both locally and on server. But the mapping function is not reusable. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0, 0), (0, 1, 0), (0, 0, 1), (0, 1, 0), (1, 0, 0), (0, 0, 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution 2: broadcast the map to all the nodes.\n",
    "# These variables will be read-only.\n",
    "bcast_map = sc.broadcast(one_hot_encoding)\n",
    "\n",
    "def bcast_map_ohe(x, shared_ohe):\n",
    "    return shared_ohe[x]\n",
    "\n",
    "# Use the bcast_map_ohe function to call the broadcasted variable bcast_map.\n",
    "(sc.parallelize([\"M\", \"F\", \"U\", \"F\", \"M\", \"U\"])\n",
    " .map(lambda x: bcast_map_ohe(x, bcast_map.value))\n",
    " .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To remove a broadcasted variable, use unpersist on the variable. This will also free up memory\n",
    "bcast_map.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators: write-only variables ##\n",
    "\n",
    "Only the driver node and node running the IPython notebook can read its value. The other nodes cannot.\n",
    "\n",
    "In the following example, the objective is to count the number of empty lines in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Solution 1:\n",
    "2 Spark jobs-\n",
    "a. Read the text file.\n",
    "b. Filter the empty lines\n",
    "c. Count them\n",
    "'''\n",
    "(sc.textFile('file:///home/vagrant/datasets/hadoop_git_readme.txt')\n",
    "   .filter(lambda line: len(line) == 0)\n",
    "   .count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty lines in the hadoop git readme file is :  6\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution 2:\n",
    "a. Define accumulator variable and set it to 0\n",
    "b. Add 1 to it for each empty line that we find in the file.\n",
    "'''\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def split_line(line):\n",
    "    if len(line) == 0:\n",
    "        accum.add(1)\n",
    "    return 1\n",
    "\n",
    "tot_lines = (sc.textFile('file:///home/vagrant/datasets/hadoop_git_readme.txt')\n",
    "                .map(split_line)\n",
    "                .count())\n",
    "\n",
    "empty_lines = accum.value\n",
    "print 'Number of empty lines in the hadoop git readme file is : ', empty_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast and Accumulators ##\n",
    "\n",
    "### Example using the iris dataset ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps we will follow are as follows:\n",
    "\n",
    "* Load the dataset and broadcast to all nodes.\n",
    "* Each node will use a different classifier on the dataset and return the classifier name and its accuracy score on the full dataset.\n",
    "* If a classifier raises an exception, store the error in an accumulator.\n",
    "* Final output will be a list of classifiers that didnt have errors and their corresponding accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset and broadcast to all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Broadcasting dataset to all nodes\n",
    "bcast_dataset = sc.broadcast(load_iris())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create accumulator object that stores a tuple (classifier_name, exception_raised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import AccumulatorParam\n",
    "\n",
    "class ErrorAccumulator(AccumulatorParam):\n",
    "    def zero(self, initialList):\n",
    "        return initialList\n",
    "    \n",
    "    # Make sure the elements are list elements so they can be added\n",
    "    def addInPlace(self, v1, v2):\n",
    "        if not isinstance(v1, list):\n",
    "            v1 = [v1]\n",
    "        if not isinstance(v2, list):\n",
    "            v2 = [v2]\n",
    "        return v1 + v2\n",
    "\n",
    "errAccum = sc.accumulator([], ErrorAccumulator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define mapping function ###\n",
    "\n",
    "It will perform the following:\n",
    "\n",
    "* Each node will train, test and evaluate the classifier on the broadcasted Iris dataset.\n",
    "* The function will recieve the classifier object as an argument.\n",
    "* It will return a tuple containing the classifier name and its accuracy score. \n",
    "* If an exception is raised the classifier name and exception are added to the accumulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mapping function\n",
    "def apply_classifier(clf, dataset):\n",
    "    # Define classifier and feature and target variables\n",
    "    clf_name = clf.__class__.__name__\n",
    "    X = dataset.value.data\n",
    "    y = dataset.value.target\n",
    "    \n",
    "    try:\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        # Fit the classifier and make predictions\n",
    "        clf.fit(X, y)\n",
    "        y_pred = clf.predict(X)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "\n",
    "        return [(clf_name, acc)]\n",
    "    \n",
    "    # If exception, classifier name and exception are added to the accumulator\n",
    "    except Exception as e:\n",
    "        errAccum.add((clf_name, str(e)))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply classifiers to the iris dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DummyClassifier', 0.33333333333333331),\n",
       " ('SGDClassifier', 0.66666666666666663)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# We will try the following classifiers\n",
    "classifiers = [DummyClassifier('most_frequent'), \n",
    "               SGDClassifier(), \n",
    "               PCA(), \n",
    "               MDS()]\n",
    "\n",
    "# Use flatmap() to collect just the outputs of the mappers that didn't catch an exception\n",
    "# bcast_dataset is loaded in Step 1\n",
    "(sc.parallelize(classifiers)\n",
    "     .flatMap(lambda x: apply_classifier(x, bcast_dataset))\n",
    "     .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The errors are: [('PCA', \"'PCA' object has no attribute 'predict'\"), ('MDS', \"Proximity must be 'precomputed' or 'euclidean'. Got euclidean instead\")]\n"
     ]
    }
   ],
   "source": [
    "# Find the classifiers that raised an exception\n",
    "print \"The errors are:\", errAccum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Free memory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bcast_dataset.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Preprocessing in Spark #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To load JSON compliant files we create an SQL context\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"user_id\":0, \"balance\": 10.0}\r\n",
      "{\"user_id\":1, \"gender\":\"M\", \"balance\": 1.0}\r\n",
      "{\"user_id\":2, \"gender\":\"F\", \"balance\": -0.5}\r\n",
      "{\"user_id\":3, \"gender\":\"F\", \"balance\": 0.0}\r\n",
      "{\"user_id\":4, \"balance\": 5.0}\r\n",
      "{\"user_id\":5, \"gender\":\"M\", \"balance\": 3.0}"
     ]
    }
   ],
   "source": [
    "# View sample JSON file \n",
    "!cat /home/vagrant/datasets/users.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|   10.0|  null|      0|\n",
      "|    1.0|     M|      1|\n",
      "|   -0.5|     F|      2|\n",
      "|    0.0|     F|      3|\n",
      "|    5.0|  null|      4|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using .read.json('file_name')\n",
    "df = sqlContext.read.json(\"file:///home/vagrant/datasets/users.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform computations using filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- balance: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of the JSON file\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|    1.0|     M|      1|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using filter using dataframe operations\n",
    "(df.filter(df['gender'] != 'null')\n",
    "   .filter(df['balance'] > 0)\n",
    "   .select(['balance', 'gender', 'user_id'])\n",
    "   .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|    1.0|     M|      1|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using filter with SQL query \n",
    "(df.filter('gender is not null')\n",
    "   .filter('balance > 0').select(\"*\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|    1.0|     M|      1|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One line\n",
    "df.filter('gender is not null and balance > 0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing data ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|    1.0|     M|      1|\n",
      "|   -0.5|     F|      2|\n",
      "|    0.0|     F|      3|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing values\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|    1.0|     M|      1|\n",
      "|   -0.5|     F|      2|\n",
      "|    0.0|     F|      3|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where we have missing values in the 'gender' column\n",
    "df.na.drop(subset=[\"gender\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|   10.0|     U|      0|\n",
      "|    1.0|     M|      1|\n",
      "|   -0.5|     F|      2|\n",
      "|    0.0|     F|      3|\n",
      "|    5.0|     U|      4|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set values for missing values in the gender and balance column\n",
    "df.na.fill({'gender': \"U\", 'balance': 0.0}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping and creating tables in memory ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|gender|avg(balance)|\n",
      "+------+------------+\n",
      "|     F|       -0.25|\n",
      "|     M|         2.0|\n",
      "|     U|         7.5|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill in default values for missing values for the gender and balance column\n",
    "# Then, group by the gender column and average out the values in the balance column for each gender\n",
    "(df.na.fill({'gender': \"U\", 'balance': 0.0})\n",
    "   .groupBy(\"gender\").avg('balance').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Dataframe as an SQL table ###\n",
    "\n",
    "#### Running SQL commands ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register datafram as SQL table\n",
    "df.registerTempTable(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|  _c1|\n",
      "+------+-----+\n",
      "|     F|-0.25|\n",
      "|     M|  2.0|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL command to find average balance for each gender for rows which don't have NULL values\n",
    "sqlContext.sql(\"\"\"\n",
    "    SELECT gender, AVG(balance) \n",
    "    FROM users \n",
    "    WHERE gender IS NOT NULL \n",
    "    GROUP BY gender\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking type of table\n",
    "type(sqlContext.table(\"users\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(balance=10.0, gender=None, user_id=0),\n",
       " Row(balance=1.0, gender=u'M', user_id=1),\n",
       " Row(balance=-0.5, gender=u'F', user_id=2),\n",
       " Row(balance=0.0, gender=u'F', user_id=3),\n",
       " Row(balance=5.0, gender=None, user_id=4),\n",
       " Row(balance=3.0, gender=u'M', user_id=5)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the full table\n",
    "sqlContext.table(\"users\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(balance=10.0, gender=None, user_id=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First row\n",
    "a_row = sqlContext.sql(\"SELECT * FROM users\").first()\n",
    "a_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Options to extract column values\n",
    "print a_row['balance']\n",
    "print a_row.balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balance': 10.0, 'gender': None, 'user_id': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_row.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write RDD to disk ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /tmp/complete_users*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write dataframe to disk\n",
    "(df.na.drop().write\n",
    "   .save(\"file:///tmp/complete_users.json\", format='json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28\r\n",
      "4 drwxrwxr-x 2 vagrant vagrant 4096 Oct  7 21:01 .\r\n",
      "4 drwxrwxrwt 9 root    root    4096 Oct  7 21:01 ..\r\n",
      "4 -rw-r--r-- 1 vagrant vagrant   83 Oct  7 21:01 part-r-00000-8a9e7ba8-f31e-4e37-818d-8a5fca989427\r\n",
      "4 -rw-rw-r-- 1 vagrant vagrant   12 Oct  7 21:01 .part-r-00000-8a9e7ba8-f31e-4e37-818d-8a5fca989427.crc\r\n",
      "4 -rw-r--r-- 1 vagrant vagrant   82 Oct  7 21:01 part-r-00001-8a9e7ba8-f31e-4e37-818d-8a5fca989427\r\n",
      "4 -rw-rw-r-- 1 vagrant vagrant   12 Oct  7 21:01 .part-r-00001-8a9e7ba8-f31e-4e37-818d-8a5fca989427.crc\r\n",
      "0 -rw-r--r-- 1 vagrant vagrant    0 Oct  7 21:01 _SUCCESS\r\n",
      "4 -rw-rw-r-- 1 vagrant vagrant    8 Oct  7 21:01 ._SUCCESS.crc\r\n"
     ]
    }
   ],
   "source": [
    "# Check local filesystem\n",
    "!ls -als /tmp/complete_users.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+\n",
      "|balance|gender|user_id|\n",
      "+-------+------+-------+\n",
      "|    1.0|     M|      1|\n",
      "|   -0.5|     F|      2|\n",
      "|    0.0|     F|      3|\n",
      "|    3.0|     M|      5|\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View JSON file using SQL\n",
    "sqlContext.sql(\n",
    "    \"SELECT * FROM json.`file:///tmp/complete_users.json`\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet data format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write data onto local system in parquet format\n",
    "df.na.drop().write.save(\n",
    "    \"file:///tmp/complete_users.parquet\", format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\r\n",
      "4 drwxrwxr-x  2 vagrant vagrant 4096 Oct  7 21:31 .\r\n",
      "4 drwxrwxrwt 10 root    root    4096 Oct  7 21:31 ..\r\n",
      "4 -rw-r--r--  1 vagrant vagrant  376 Oct  7 21:31 _common_metadata\r\n",
      "4 -rw-rw-r--  1 vagrant vagrant   12 Oct  7 21:31 ._common_metadata.crc\r\n",
      "4 -rw-r--r--  1 vagrant vagrant 1082 Oct  7 21:31 _metadata\r\n",
      "4 -rw-rw-r--  1 vagrant vagrant   20 Oct  7 21:31 ._metadata.crc\r\n",
      "4 -rw-r--r--  1 vagrant vagrant  750 Oct  7 21:31 part-r-00000-c36fe8f5-3523-4ed5-8a31-570c610dd3fb.gz.parquet\r\n",
      "4 -rw-rw-r--  1 vagrant vagrant   16 Oct  7 21:31 .part-r-00000-c36fe8f5-3523-4ed5-8a31-570c610dd3fb.gz.parquet.crc\r\n",
      "4 -rw-r--r--  1 vagrant vagrant  746 Oct  7 21:31 part-r-00001-c36fe8f5-3523-4ed5-8a31-570c610dd3fb.gz.parquet\r\n",
      "4 -rw-rw-r--  1 vagrant vagrant   16 Oct  7 21:31 .part-r-00001-c36fe8f5-3523-4ed5-8a31-570c610dd3fb.gz.parquet.crc\r\n",
      "0 -rw-r--r--  1 vagrant vagrant    0 Oct  7 21:31 _SUCCESS\r\n",
      "4 -rw-rw-r--  1 vagrant vagrant    8 Oct  7 21:31 ._SUCCESS.crc\r\n"
     ]
    }
   ],
   "source": [
    "# View the files on the local filesystem\n",
    "!ls -als /tmp/complete_users.parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark dataframe from an existing RDD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a row object for every record in the RDD\n",
    "rdd_gender = \\\n",
    "    sc.parallelize([Row(short_gender=\"M\", long_gender=\"Male\"),\n",
    "                    Row(short_gender=\"F\", long_gender=\"Female\")])\n",
    "\n",
    "# Create the dataframe \n",
    "(sqlContext.createDataFrame(rdd_gender)\n",
    "           .registerTempTable(\"gender_maps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|long_gender|short_gender|\n",
      "+-----------+------------+\n",
      "|       Male|           M|\n",
      "|     Female|           F|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the table\n",
    "sqlContext.table(\"gender_maps\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|balance|long_gender|user_id|\n",
      "+-------+-----------+-------+\n",
      "|    3.0|       Male|      5|\n",
      "|    1.0|       Male|      1|\n",
      "|    0.0|     Female|      3|\n",
      "|   -0.5|     Female|      2|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# JOIN parquet file and newly created dataframe\n",
    "sqlContext.sql(\"\"\"\n",
    "    SELECT balance, long_gender, user_id \n",
    "    FROM parquet.`file:///tmp/complete_users.parquet` \n",
    "    JOIN gender_maps ON gender=short_gender\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing memory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'gender_maps', u'users']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tables currently in-memory\n",
    "sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop tables in memory using dropTempTable\n",
    "for table in sqlContext.tableNames():\n",
    "    sqlContext.dropTempTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
