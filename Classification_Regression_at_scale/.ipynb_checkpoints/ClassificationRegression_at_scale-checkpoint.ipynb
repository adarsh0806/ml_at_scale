{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Extremely Randomized Forest #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adarshnair/Desktop/PyML_at_scale/Classification_Regression_at_scale\n",
      "ExtraTreesClassifier -> cross validation accuracy: mean = 0.812 std = 0.003\n",
      "[[6610  448]\n",
      " [1238  704]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.81266666666666665"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xlrd\n",
    "import urllib\n",
    "\n",
    "# Set your path here\n",
    "cwd = os.getcwd()\n",
    "print cwd\n",
    "os.chdir(cwd)\n",
    "\n",
    "# Read the data\n",
    "target = 'default payment next month'\n",
    "data = pd.read_excel('defaultcredit.xls', skiprows = 1)\n",
    "\n",
    "# Define the target variable and features\n",
    "target = 'default payment next month'\n",
    "y = np.asarray(data[target])\n",
    "features = data.columns.drop(['ID', target])\n",
    "X = np.asarray(data[features])\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# ExtraTreesClassifier: splitting the data and fitting on training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=101)\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=500, \n",
    "                           random_state=101)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "scores = cross_val_score(clf, \n",
    "                         X_train, \n",
    "                         y_train, \n",
    "                         cv=3,\n",
    "                         scoring='accuracy', \n",
    "                         n_jobs=-1)\n",
    "\n",
    "print \"ExtraTreesClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f\" % (np.mean(scores), np.std(scores))\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred=clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "'''\n",
    "a is the number of correct predictions that an instance is negative, (TN)\n",
    "b is the number of incorrect predictions that an instance is positive, (FP)\n",
    "c is the number of incorrect of predictions that an instance negative, (FN)\n",
    "d is the number of correct predictions that an instance is positive. (TP)\n",
    "\n",
    "a b \n",
    "c d\n",
    "'''\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "print confusionMatrix\n",
    "\n",
    "# Get the accuracy score on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Fast parameter optimization with randomized search ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "           max_depth=12, max_features=22, max_leaf_nodes=None,\n",
      "           min_samples_leaf=2, min_samples_split=8,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "           oob_score=False, random_state=101, verbose=0, warm_start=False)\n",
      "[[6718  340]\n",
      " [1245  697]]\n",
      "0.823888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Create parameter distribution\n",
    "param_dist = {\"max_depth\": [1,3, 7,8,12,None],\n",
    "             \"max_features\": [8,9,10,11,16,22],\n",
    "             \"min_samples_split\": [8,10,11,14,16,19],\n",
    "             \"min_samples_leaf\": [1,2,3,4,5,6,7],\n",
    "             \"bootstrap\": [True, False]}\n",
    "\n",
    "# We use only 25 random parameter \n",
    "# valuations but we manage to keep training times in check.\n",
    "rsearch = RandomizedSearchCV(clf, \n",
    "                             param_distributions = param_dist,\n",
    "                             n_iter=25)  \n",
    "\n",
    "# Fit the data\n",
    "rsearch.fit(X_train,y_train)\n",
    "rsearch.grid_scores_\n",
    "\n",
    "# Choose the best estimator\n",
    "bestclf = rsearch.best_estimator_\n",
    "print bestclf\n",
    "\n",
    "# Make predictions using the best estimator\n",
    "y_pred = bestclf.predict(X_test)\n",
    "\n",
    "\n",
    "# Get performance metrics\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "print confusionMatrix \n",
    "\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have increased our accuracy by using RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Using subsampling for using Extremely Randomized Forest on large datasets #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Fetch the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adarshnair/Desktop/PyML_at_scale/Classification_Regression_at_scale\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import os\n",
    "\n",
    "# Set current path\n",
    "cwd = os.getcwd()\n",
    "print cwd\n",
    "os.chdir(cwd)\n",
    "\n",
    "dataset = fetch_covtype(random_state=111, shuffle=True)\n",
    "dataset = fetch_covtype()\n",
    "\n",
    "# Set the feature and target variables\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0)\n",
    "del(X,y)\n",
    "covtrain = np.c_[X_train,y_train]\n",
    "covtest = np.c_[X_test,y_test]\n",
    "np.savetxt('covtrain.csv', covtrain, delimiter=\",\")\n",
    "np.savetxt('covtest.csv', covtest, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Split the data into 3 batches ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "$ subsample --reservoir -n 10000 covtrain.csv > cov1.csv\n",
    "$ subsample --reservoir -n 10000 covtrain.csv > cov2.csv\n",
    "$ subsample --reservoir -n 10000 covtrain.csv > cov3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3: Load each subsample and fit ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier -> cross validation accuracy: mean = 0.802 std = 0.006\n",
      "[ 0.80665468  0.79351741  0.80576403]\n",
      "amount of trees in the model: 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#here we load sample 1 - 100 trees\n",
    "df = pd.read_csv('cov1.csv')\n",
    "y=df[df.columns[54]]\n",
    "X=df[df.columns[0:54]]\n",
    "\n",
    "clf1=ExtraTreesClassifier(n_estimators=100, \n",
    "                          random_state=101,\n",
    "                          warm_start=True)\n",
    "clf1.fit(X,y)\n",
    "scores = cross_val_score(clf1, \n",
    "                         X, \n",
    "                         y, \n",
    "                         cv=3,\n",
    "                         scoring='accuracy', \n",
    "                         n_jobs=-1)\n",
    "print \"ExtraTreesClassifier -> cross validation accuracy: mean = %0.3f std = %0.3f\" % (np.mean(scores), np.std(scores))\n",
    "print scores\n",
    "print 'amount of trees in the model: %s' % len(clf1.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier after params -> cross validation accuracy: mean = 0.798 std = 0.001\n",
      "[ 0.79736211  0.80042017  0.79735815]\n",
      "amount of trees in the model: 150\n"
     ]
    }
   ],
   "source": [
    "# sample 2 - 150 trees\n",
    "df = pd.read_csv('cov2.csv')\n",
    "y=df[df.columns[54]]\n",
    "X=df[df.columns[0:54]]\n",
    "\n",
    "clf1.set_params(n_estimators=150, \n",
    "                random_state=101,\n",
    "                warm_start=True)\n",
    "clf1.fit(X,y)\n",
    "scores = cross_val_score(clf1, \n",
    "                         X, \n",
    "                         y, \n",
    "                         cv=3,\n",
    "                         scoring='accuracy', \n",
    "                         n_jobs=-1)\n",
    "print \"ExtraTreesClassifier after params -> cross validation accuracy: mean = %0.3f std = %0.3f\" % (np.mean(scores), np.std(scores))\n",
    "print scores\n",
    "print 'amount of trees in the model: %s' % len(clf1.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier after params -> cross validation accuracy: mean = 0.811 std = 0.001\n",
      "[ 0.81000899  0.81332533  0.81111111]\n",
      "amount of trees in the model: 200\n"
     ]
    }
   ],
   "source": [
    "# sample 3\n",
    "df = pd.read_csv('cov3.csv')\n",
    "y=df[df.columns[54]]\n",
    "X=df[df.columns[0:54]]\n",
    "\n",
    "clf1.set_params(n_estimators=200, \n",
    "                random_state=101,\n",
    "                warm_start=True)\n",
    "clf1.fit(X,y)\n",
    "scores = cross_val_score(clf1, \n",
    "                         X, \n",
    "                         y, \n",
    "                         cv=3,\n",
    "                         scoring='accuracy', \n",
    "                         n_jobs=-1)\n",
    "print \"ExtraTreesClassifier after params -> cross validation accuracy: mean = %0.3f std = %0.3f\" % (np.mean(scores), np.std(scores))\n",
    "print scores\n",
    "print 'amount of trees in the model: %s' % len(clf1.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test score 0.92185447181058278\n"
     ]
    }
   ],
   "source": [
    "# Now let’s predict our combined model on the test set and check our score.\n",
    "df = pd.read_csv('covtest.csv')\n",
    "X = df[df.columns[0:54]]\n",
    "y = df[df.columns[54]]\n",
    "\n",
    "pred2 = clf1.predict(X)\n",
    "scores = cross_val_score(clf1, \n",
    "                         X, \n",
    "                         y, \n",
    "                         cv=3,\n",
    "                         scoring='accuracy', \n",
    "                         n_jobs=-1)\n",
    "print \"final test score %r\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Gradient Boosting Classifier #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: GBM without warm_start ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'address', 'all', '3d', 'our', 'over', 'remove', 'internet', 'order', 'mail', 'receive', 'will', 'people', 'report', 'addresses', 'free', 'business', 'email', 'you', 'credit', 'your', 'font', '000', 'money', 'hp', 'hpl', 'george', '650', 'lab', 'labs', 'telnet', '857', 'data', '415', '85', 'technology', '1999', 'parts', 'pm', 'direct', 'cs', 'meeting', 'original', 'project', 're', 'edu', 'table', 'conference', 'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest', 'capital_run_length_total']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>address</th>\n",
       "      <th>all</th>\n",
       "      <th>3d</th>\n",
       "      <th>our</th>\n",
       "      <th>over</th>\n",
       "      <th>remove</th>\n",
       "      <th>internet</th>\n",
       "      <th>order</th>\n",
       "      <th>mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   make  address   all   3d   our  over  remove  internet  order  mail  ...   \\\n",
       "0  0.00     0.64  0.64  0.0  0.32  0.00    0.00      0.00   0.00  0.00  ...    \n",
       "1  0.21     0.28  0.50  0.0  0.14  0.28    0.21      0.07   0.00  0.94  ...    \n",
       "2  0.06     0.00  0.71  0.0  1.23  0.19    0.19      0.12   0.64  0.25  ...    \n",
       "3  0.00     0.00  0.00  0.0  0.63  0.00    0.31      0.63   0.31  0.63  ...    \n",
       "4  0.00     0.00  0.00  0.0  0.63  0.00    0.31      0.63   0.31  0.63  ...    \n",
       "\n",
       "   char_freq_;  char_freq_(  char_freq_[  char_freq_!  char_freq_$  \\\n",
       "0         0.00        0.000          0.0        0.778        0.000   \n",
       "1         0.00        0.132          0.0        0.372        0.180   \n",
       "2         0.01        0.143          0.0        0.276        0.184   \n",
       "3         0.00        0.137          0.0        0.137        0.000   \n",
       "4         0.00        0.135          0.0        0.135        0.000   \n",
       "\n",
       "   char_freq_#  capital_run_length_average  capital_run_length_longest  \\\n",
       "0        0.000                       3.756                          61   \n",
       "1        0.048                       5.114                         101   \n",
       "2        0.010                       9.821                         485   \n",
       "3        0.000                       3.537                          40   \n",
       "4        0.000                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import urllib2\n",
    "import urllib2\n",
    "from sklearn import ensemble\n",
    "\n",
    "# Grab data\n",
    "columnNames1_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'\n",
    "columnNames1 = [\n",
    "   line.strip().split(':')[0]\n",
    "   for line in urllib2.urlopen(columnNames1_url).readlines()[33:]]\n",
    "\n",
    "# Change column names\n",
    "columnNames1\n",
    "n = 0\n",
    "for i in columnNames1:\n",
    "   columnNames1[n] = i.replace('word_freq_','')\n",
    "   n += 1\n",
    "print columnNames1\n",
    "\n",
    "# Data preprocessing\n",
    "spamdata = pandas.read_csv(\n",
    "   'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data',\n",
    "   header=None, names=(columnNames1 + ['spam'])\n",
    ")\n",
    "\n",
    "# Set feature and target variables\n",
    "# All rows, all eclumns except column 58\n",
    "X = spamdata.values[:,:57]\n",
    "y = spamdata['spam']\n",
    "\n",
    "spamdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores mean value:  0.945030177548\n",
      "[[799  36]\n",
      " [ 63 483]]\n",
      "Accuracy score:  0.928312816799\n",
      "Show feature relevance:  [  3.92048783e-03   3.60027473e-03   9.74413716e-03   2.65435429e-03\n",
      "   1.20586557e-02   4.23505596e-03   1.32726979e-02   2.06756938e-02\n",
      "   3.63957053e-03   1.49384505e-02   8.96946481e-03   3.32963571e-02\n",
      "   2.33324239e-03   9.45292434e-02   3.96500865e-04   1.69613556e-02\n",
      "   4.02531761e-02   1.12791975e-02   4.67209613e-02   3.68033142e-03\n",
      "   2.75038519e-02   8.06681367e-04   2.59315600e-02   5.56167797e-03\n",
      "   1.09143309e-02   3.81396392e-03   3.57384108e-03   4.89295950e-03\n",
      "   6.71863955e-04   2.56619971e-03   1.48475132e-04   2.50652925e-04\n",
      "   2.80756735e-03   2.19083910e-05   1.09440947e-03   4.69189420e-03\n",
      "   5.13302322e-03   4.36303813e-04   1.94261505e-03   6.08174726e-04\n",
      "   2.71902886e-04   2.27073239e-03   5.00886660e-03   4.80224592e-03\n",
      "   1.00947737e-02   1.88114627e-02   1.03080853e-04   5.29809743e-04\n",
      "   2.26245798e-01   2.57178324e-02   1.59664784e-03   3.97195359e-02\n",
      "   2.35279922e-02   1.37178940e-03   6.37079524e-02   2.95395124e-02\n",
      "   4.61489006e-02]\n",
      "Show feature relevance in order: [[ 0.     0.   ]\n",
      " [ 0.     0.21 ]\n",
      " [ 0.01   0.   ]\n",
      " ..., \n",
      " [ 0.102  0.   ]\n",
      " [ 0.     0.   ]\n",
      " [ 0.     0.   ]]\n",
      "[(0.2262, 'char_freq_;'),\n",
      " (0.0945, 'report'),\n",
      " (0.0637, 'capital_run_length_average'),\n",
      " (0.0467, 'you'),\n",
      " (0.0461, 'capital_run_length_total'),\n",
      " (0.0403, 'business'),\n",
      " (0.0397, 'char_freq_!'),\n",
      " (0.0333, 'will'),\n",
      " (0.0295, 'capital_run_length_longest'),\n",
      " (0.0275, 'your'),\n",
      " (0.0259, '000'),\n",
      " (0.0257, 'char_freq_('),\n",
      " (0.0235, 'char_freq_$'),\n",
      " (0.0207, 'internet'),\n",
      " (0.0188, 'edu'),\n",
      " (0.017, 'free'),\n",
      " (0.0149, 'mail'),\n",
      " (0.0133, 'remove'),\n",
      " (0.0121, 'our'),\n",
      " (0.0113, 'email'),\n",
      " (0.0109, 'hp'),\n",
      " (0.0101, 're'),\n",
      " (0.0097, 'all'),\n",
      " (0.009, 'receive'),\n",
      " (0.0056, 'money'),\n",
      " (0.0051, '1999'),\n",
      " (0.005, 'original'),\n",
      " (0.0049, '650'),\n",
      " (0.0048, 'project'),\n",
      " (0.0047, 'technology'),\n",
      " (0.0042, 'over'),\n",
      " (0.0039, 'make'),\n",
      " (0.0038, 'hpl'),\n",
      " (0.0037, 'credit'),\n",
      " (0.0036, 'order'),\n",
      " (0.0036, 'george'),\n",
      " (0.0036, 'address'),\n",
      " (0.0028, 'data'),\n",
      " (0.0027, '3d'),\n",
      " (0.0026, 'labs'),\n",
      " (0.0023, 'people'),\n",
      " (0.0023, 'meeting'),\n",
      " (0.0019, 'pm'),\n",
      " (0.0016, 'char_freq_['),\n",
      " (0.0014, 'char_freq_#'),\n",
      " (0.0011, '85'),\n",
      " (0.0008, 'font'),\n",
      " (0.0007, 'lab'),\n",
      " (0.0006, 'direct'),\n",
      " (0.0005, 'conference'),\n",
      " (0.0004, 'parts'),\n",
      " (0.0004, 'addresses'),\n",
      " (0.0003, 'cs'),\n",
      " (0.0003, '857'),\n",
      " (0.0001, 'telnet'),\n",
      " (0.0001, 'table'),\n",
      " (0.0, '415')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics  import recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=22)\n",
    "\n",
    "# Define the classifier\n",
    "clf = ensemble.GradientBoostingClassifier(n_estimators = 300,\n",
    "                                          random_state= 222,\n",
    "                                          max_depth = 16,\n",
    "                                          learning_rate= .1,\n",
    "                                          subsample= .5)\n",
    "# Fit the classifier on the training data\n",
    "scores= clf.fit(X_train,y_train)\n",
    "\n",
    "# Find scores for all the cross validated combinations\n",
    "scores2 = cross_val_score(clf, \n",
    "                          X_train, \n",
    "                          y_train, \n",
    "                          cv = 3, \n",
    "                          scoring = 'accuracy',\n",
    "                          n_jobs = -1)\n",
    "\n",
    "print \"Scores mean value: \", scores2.mean()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = cross_val_predict(clf, \n",
    "                           X_test, \n",
    "                           y_test, \n",
    "                           cv = 10)\n",
    "\n",
    "\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "print confusionMatrix\n",
    "\n",
    "print \"Accuracy score: \", accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Show feature relevance\n",
    "print \"Show feature relevance: \", clf.feature_importances_\n",
    "\n",
    "\n",
    "def featureImp_order(clf, X, k=5):\n",
    "    return X[:,clf.feature_importances_.argsort()[::-1][:k]]\n",
    "newX = featureImp_order(clf,X,2)\n",
    "\n",
    "print \"Show feature relevance in order:\", newX\n",
    "\n",
    "# let's order the features in amount of importance\n",
    "import pprint\n",
    "pprint.pprint(sorted(zip(map(lambda x: round(x, 4), clf.feature_importances_), columnNames1),\n",
    "            reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: GBM with warm_start ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** warm_start ** allows for storing new tree information after each iteration, and is added to the previous tree without generating new trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94       835\n",
      "          1       0.92      0.89      0.91       546\n",
      "\n",
      "avg / total       0.93      0.93      0.93      1381\n",
      "\n",
      "\n",
      "GBM parameters: \n",
      "<bound method GradientBoostingClassifier.set_params of GradientBoostingClassifier(init=None, learning_rate=0.05, loss='deviance',\n",
      "              max_depth=20, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1401,\n",
      "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
      "              warm_start=True)>\n"
     ]
    }
   ],
   "source": [
    "# with warm_start\n",
    "gbc = GradientBoostingClassifier(warm_start = True, \n",
    "                                 learning_rate = .05, \n",
    "                                 max_depth = 20,\n",
    "                                 random_state = 0)\n",
    "\n",
    "# Build tree models incrementally\n",
    "for n_estimators in range(1, 1500, 100):\n",
    "    gbc.set_params(n_estimators = n_estimators)\n",
    "    gbc.fit(X_train, y_train) \n",
    "\n",
    "# Make predictions on warm started Gradient Boosting Machine\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "print \"Classification results: \\n\", classification_report(y_test, y_pred)\n",
    "print \"\\nGBM parameters: \\n\", gbc.set_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Storing the model for later use after training ##\n",
    "\n",
    "To store the model and use it later, we can use `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adarshnair/Desktop/PyML_at_scale/Classification_Regression_at_scale\n",
      "[1 1 1 ..., 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import errno\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print cwd\n",
    "os.chdir(cwd)\n",
    "\n",
    "# set your path here to store the pickled model\n",
    "path = cwd + '/clf'\n",
    "\n",
    "# create directory\n",
    "clfm = os.makedirs(path)\n",
    "os.chdir(path)\n",
    "\n",
    "# now let's load our stored model and use it for prediction.\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# joblib dumps the model in the form of a pickle\n",
    "joblib.dump( gbc,'clf_gbc.pkl')\n",
    "\n",
    "# load the model\n",
    "model_clone = joblib.load('clf_gbc.pkl')\n",
    "# make predictions with the loaded model\n",
    "zpred = model_clone.predict(X_test)\n",
    "\n",
    "print zpred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Extreme Gradient Boosting(XGBoost)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of XGBoost:\n",
    "\n",
    "* XGBoost can leverage multithreading on a single machine and parallel processing on clusters of multiple servers(sharding) while GBM(Gradient boosting machine) has no options for parallel processing.\n",
    "* XGBoost can handle sparse data(GBM cannot) as input without storing zero values in memory.\n",
    "* Best node splits are calculated with better efficiency than GBM through a process called quantile sketch. This method transforms the data by a weighting algorithm so the candidate splits are sorted based on a certain accuracy level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.1: Classification - XGBoost on spam dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Define XGBoost classifer\n",
    "clf = xgb.XGBClassifier(n_estimators=100,\n",
    "                        max_depth=8,\n",
    "                        learning_rate=.1,\n",
    "                        subsample=.5)\n",
    "\n",
    "# Gradient boosting machine (GBM)\n",
    "clf1 = GradientBoostingClassifier(n_estimators=100,\n",
    "                                  max_depth=8,\n",
    "                                  learning_rate=.1,\n",
    "                                  subsample=.5)\n",
    "\n",
    "# Fit XGBoost\n",
    "xgm = clf.fit(X_train,y_train)\n",
    "\n",
    "# Fit GBM\n",
    "gbmf = clf1.fit(X_train,y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = xgm.predict(X_test)\n",
    "y_pred2 = gbmf.predict(X_test)\n",
    "\n",
    "print 'XGBoost results %r' % (classification_report(y_test, y_pred))\n",
    "print 'gbm results %r' % (classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2 Regression - XGBoost on California housing dataset ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2.a: XGBoost regressor without parameter grid ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pd=fetch_california_housing()\n",
    "\n",
    "# Log the target variable to even out skews\n",
    "y = np.log(pd.target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(pd.data,\n",
    "                                                    y,\n",
    "                                                    test_size=0.15,\n",
    "                                                    random_state=111)\n",
    "\n",
    "# Define features\n",
    "names = pd.feature_names\n",
    "print names\n",
    "\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Define XGBoost Regressor\n",
    "clf = xgb.XGBRegressor(gamma = 0,\n",
    "                       objective = \"reg:linear\",\n",
    "                       nthread = -1)\n",
    "\n",
    "\n",
    "# Fit the regressor and make predictions\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print 'score before gridsearch %r' % mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2.b: XGBoost regressor with parameter grid ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up parameter grid\n",
    "params = {'max_depth':[4,6,8],\n",
    "          'n_estimators':[1000],\n",
    "          'min_child_weight':range(1,3),\n",
    "          'learning_rate':[.1,.01,.001],\n",
    "          'colsample_bytree':[.8,.9,1],\n",
    "          'gamma':[0,1]}\n",
    "\n",
    "# With the parameter nthread we specify XGBoost for parallelisation \n",
    "cvx = xgb.XGBRegressor(objective= \"reg:linear\",\n",
    "                       nthread=-1)\n",
    "clf = GridSearchCV(estimator = cvx,\n",
    "                   param_grid = params,\n",
    "                   n_jobs = -1,\n",
    "                   scoring = 'mean_absolute_error',\n",
    "                   verbose = True)\n",
    "\n",
    "# Fit the regressor and make predictions\n",
    "clf.fit(X_train,y_train)\n",
    "print clf.best_params_\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print 'score after gridsearch %r' %mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.3: Feature selection with XGBoost ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pylab as plt\n",
    "%matplotlib inline   \n",
    "\n",
    "#our best parameter set \n",
    "# {'colsample_bytree': 1, 'learning_rate': 0.1, 'min_child_weight': 1, 'n_estimators': 500, #'max_depth': 8, 'gamma': 0}\n",
    "\n",
    "params={'objective': \"reg:linear\",\n",
    "        'eval_metric': 'rmse',\n",
    "        'eta': 0.1,\n",
    "        'max_depth':8,\n",
    "        'min_samples_leaf':4,\n",
    "        'subsample':.5,\n",
    "        'gamma':0\n",
    "       }\n",
    "\n",
    "# Create the Dimension Matrix\n",
    "dm = xgb.DMatrix(X_train, \n",
    "                 label = y_train,\n",
    "                 feature_names=names)\n",
    "\n",
    "# Train the model\n",
    "regbgb = xgb.train(params, \n",
    "                   dm, \n",
    "                   num_boost_round=100)\n",
    "# Get F scores\n",
    "np.random.seed(1)\n",
    "regbgb.get_fscore()\n",
    "\n",
    "# Get the features with their F score\n",
    "regbgb.feature_names\n",
    "regbgb.get_fscore()\n",
    "xgb.plot_importance(regbgb,\n",
    "                    color='magenta',\n",
    "                    title='california-housing|variable importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# https://github.com/PacktPublishing/Large-Scale-Machine-Learning-With-Python/blob/master/Chapter%2006/Chapter_6_code.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
